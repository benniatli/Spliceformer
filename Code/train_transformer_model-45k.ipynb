{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from math import ceil\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import pickle5 as pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from glob import glob\n",
    "\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "import copy\n",
    "\n",
    "from src.train import trainModel\n",
    "#from src.dataloader import getData,spliceDataset,h5pyDataset,collate_fn\n",
    "from src.dataloader import getData,spliceDataset,h5pyDataset,getDataPointList,getDataPointListFull,DataPointFull\n",
    "from src.weight_init import keras_init\n",
    "from src.losses import categorical_crossentropy_2d\n",
    "from src.model import SpliceFormer\n",
    "from src.evaluation_metrics import print_topl_statistics,cross_entropy_2d\n",
    "from src.gpu_metrics import run_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 28 14:37:33 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:31:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             39W /  250W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          Off |   00000000:98:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             37W /  250W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCIE-40GB          Off |   00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             38W /  250W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(23673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 32\n",
    "N_GPUS = 3\n",
    "k = 2\n",
    "NUM_ACCUMULATION_STEPS=1\n",
    "# Hyper-parameters:\n",
    "# L: Number of convolution kernels\n",
    "# W: Convolution window size in each residual unit\n",
    "# AR: Atrous rate in each residual unit\n",
    "\n",
    "W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11,\n",
    "                21, 21, 21, 21, 41, 41, 41, 41])\n",
    "AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4,\n",
    "                 10, 10, 10, 10, 25, 25, 25, 25])\n",
    "BATCH_SIZE = 16*k*N_GPUS\n",
    "\n",
    "k = NUM_ACCUMULATION_STEPS*k\n",
    "\n",
    "CL = 2 * np.sum(AR*(W-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data'\n",
    "setType = 'train'\n",
    "annotation, transcriptToLabel, seqData = getData(data_dir, setType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum nucleotide context length (CL_max/2 on either side of the \n",
    "# position of interest)\n",
    "# CL_max should be an even number\n",
    "# Sequence length of SpliceAIs (SL+CL will be the input length and\n",
    "# SL will be the output length)\n",
    "\n",
    "SL=5000\n",
    "CL_max=40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert CL_max % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene, validation_gene = train_test_split(annotation['gene'].drop_duplicates(),test_size=.1,random_state=435)\n",
    "annotation_train = annotation[annotation['gene'].isin(train_gene)]\n",
    "annotation_validation = annotation[annotation['gene'].isin(validation_gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = spliceDataset(getDataPointListFull(annotation_train,transcriptToLabel,SL,CL_max,shift=SL))\n",
    "val_dataset = spliceDataset(getDataPointListFull(annotation_validation,transcriptToLabel,SL,CL_max,shift=SL))\n",
    "train_dataset.seqData = seqData\n",
    "val_dataset.seqData = seqData\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "hs = []\n",
    "learning_rate= k*1e-3\n",
    "gamma=0.5\n",
    "#final_lr = 1e-5\n",
    "#gamma = 1/(learning_rate/final_lr)**(1/5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10: 100%|████████████| 2716/2716 [55:55<00:00,  1.24s/it, a_r=0.874, d_r=0.89, loss=0.000217, r_a=0.998, r_d=0.999, r_loss=6.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10, train loss = 0.015237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 2/10: 100%|███████████| 2716/2716 [56:16<00:00,  1.24s/it, a_r=0.894, d_r=0.906, loss=0.000189, r_a=0.998, r_d=0.999, r_loss=5.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/10, train loss = 0.000190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 3/10: 100%|███████████| 2716/2716 [56:31<00:00,  1.25s/it, a_r=0.912, d_r=0.917, loss=0.000173, r_a=0.999, r_d=0.998, r_loss=4.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/10, train loss = 0.000171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 4/10: 100%|███████████| 2716/2716 [56:13<00:00,  1.24s/it, a_r=0.923, d_r=0.929, loss=0.000148, r_a=0.999, r_d=0.999, r_loss=3.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/10, train loss = 0.000159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 5/10: 100%|███████████| 2716/2716 [56:21<00:00,  1.25s/it, a_r=0.931, d_r=0.933, loss=0.000145, r_a=0.999, r_d=0.999, r_loss=2.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/10, train loss = 0.000146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 6/10: 100%|███████████| 2716/2716 [56:20<00:00,  1.24s/it, a_r=0.936, d_r=0.944, loss=0.000126, r_a=0.999, r_d=0.999, r_loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6/10, train loss = 0.000133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 7/10: 100%|███████████| 2716/2716 [56:07<00:00,  1.24s/it, a_r=0.955, d_r=0.961, loss=0.000101, r_a=0.999, r_d=0.999, r_loss=2.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7/10, train loss = 0.000108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 8/10: 100%|█████████████| 2716/2716 [56:21<00:00,  1.25s/it, a_r=0.962, d_r=0.964, loss=9.22e-5, r_a=0.999, r_d=0.999, r_loss=2.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8/10, train loss = 0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 9/10: 100%|████████████| 2716/2716 [56:19<00:00,  1.24s/it, a_r=0.964, d_r=0.969, loss=7.98e-5, r_a=0.999, r_d=0.999, r_loss=2.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/10, train loss = 0.000078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 10/10: 100%|███████████████| 2716/2716 [56:10<00:00,  1.24s/it, a_r=0.971, d_r=0.973, loss=6.61e-5, r_a=0.999, r_d=1, r_loss=2.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10/10, train loss = 0.000070\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10: 100%|████████████| 2716/2716 [56:06<00:00,  1.24s/it, a_r=0.87, d_r=0.885, loss=0.000222, r_a=0.997, r_d=0.998, r_loss=5.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10, train loss = 0.015872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 2/10: 100%|████████████| 2716/2716 [56:25<00:00,  1.25s/it, a_r=0.901, d_r=0.91, loss=0.000185, r_a=0.998, r_d=0.998, r_loss=3.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/10, train loss = 0.000193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 3/10: 100%|███████████| 2716/2716 [56:18<00:00,  1.24s/it, a_r=0.906, d_r=0.915, loss=0.000172, r_a=0.998, r_d=0.999, r_loss=2.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/10, train loss = 0.000173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 4/10: 100%|████████████| 2716/2716 [56:31<00:00,  1.25s/it, a_r=0.92, d_r=0.926, loss=0.000156, r_a=0.998, r_d=0.999, r_loss=2.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/10, train loss = 0.000159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 5/10: 100%|███████████| 2716/2716 [56:39<00:00,  1.25s/it, a_r=0.924, d_r=0.932, loss=0.000145, r_a=0.998, r_d=0.999, r_loss=1.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/10, train loss = 0.000147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 6/10: 100%|███████████| 2716/2716 [56:21<00:00,  1.25s/it, a_r=0.935, d_r=0.941, loss=0.000139, r_a=0.999, r_d=0.999, r_loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6/10, train loss = 0.000135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 7/10: 100%|████████████| 2716/2716 [56:21<00:00,  1.24s/it, a_r=0.95, d_r=0.955, loss=0.000108, r_a=0.999, r_d=0.999, r_loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7/10, train loss = 0.000109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 8/10: 100%|█████████████| 2716/2716 [56:36<00:00,  1.25s/it, a_r=0.962, d_r=0.964, loss=9.19e-5, r_a=0.999, r_d=0.999, r_loss=1.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8/10, train loss = 0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 9/10: 100%|█████████████| 2716/2716 [56:41<00:00,  1.25s/it, a_r=0.965, d_r=0.97, loss=8.16e-5, r_a=0.999, r_d=0.999, r_loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/10, train loss = 0.000080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 10/10: 100%|████████████| 2716/2716 [56:19<00:00,  1.24s/it, a_r=0.968, d_r=0.973, loss=7.1e-5, r_a=0.999, r_d=0.999, r_loss=1.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10/10, train loss = 0.000071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10: 100%|████████████| 2716/2716 [56:14<00:00,  1.24s/it, a_r=0.871, d_r=0.884, loss=0.000222, r_a=0.996, r_d=0.997, r_loss=6.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10, train loss = 0.014111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 2/10: 100%|███████████| 2716/2716 [56:14<00:00,  1.24s/it, a_r=0.899, d_r=0.911, loss=0.000178, r_a=0.998, r_d=0.998, r_loss=4.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/10, train loss = 0.000192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 3/10: 100%|███████████| 2716/2716 [56:49<00:00,  1.26s/it, a_r=0.911, d_r=0.921, loss=0.000166, r_a=0.998, r_d=0.998, r_loss=3.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/10, train loss = 0.000171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 4/10: 100%|███████████| 2716/2716 [56:57<00:00,  1.26s/it, a_r=0.918, d_r=0.929, loss=0.000163, r_a=0.999, r_d=0.999, r_loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/10, train loss = 0.000159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 5/10: 100%|███████████| 2716/2716 [57:01<00:00,  1.26s/it, a_r=0.929, d_r=0.937, loss=0.000141, r_a=0.999, r_d=0.999, r_loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/10, train loss = 0.000145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 6/10: 100%|███████████| 2716/2716 [56:07<00:00,  1.24s/it, a_r=0.932, d_r=0.939, loss=0.000136, r_a=0.999, r_d=0.999, r_loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6/10, train loss = 0.000135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 7/10: 100%|████████████| 2716/2716 [56:33<00:00,  1.25s/it, a_r=0.951, d_r=0.958, loss=0.000107, r_a=0.999, r_d=0.999, r_loss=1.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7/10, train loss = 0.000108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 8/10: 100%|████████████| 2716/2716 [56:40<00:00,  1.25s/it, a_r=0.961, d_r=0.964, loss=9.34e-5, r_a=0.999, r_d=0.999, r_loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8/10, train loss = 0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 9/10: 100%|███████████████| 2716/2716 [56:54<00:00,  1.26s/it, a_r=0.963, d_r=0.968, loss=8.06e-5, r_a=0.999, r_d=0.999, r_loss=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/10, train loss = 0.000079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 10/10: 100%|████████████| 2716/2716 [56:38<00:00,  1.25s/it, a_r=0.971, d_r=0.97, loss=6.94e-5, r_a=0.999, r_d=0.999, r_loss=1.93]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10/10, train loss = 0.000071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_nr in range(10):\n",
    "    model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2)\n",
    "    model_m.apply(keras_init)\n",
    "    model_m = model_m.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model_m = nn.DataParallel(model_m)\n",
    "    \n",
    "    modelFileName = '../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(model_nr)\n",
    "    loss = categorical_crossentropy_2d().loss\n",
    "    optimizer = torch.optim.AdamW(model_m.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    warmup = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\n",
    "    h = trainModel(model_m,modelFileName,loss,train_loader,val_loader,optimizer,scheduler,warmup,BATCH_SIZE,epochs,device,skipValidation=True,NUM_ACCUMULATION_STEPS=NUM_ACCUMULATION_STEPS)\n",
    "    hs.append(h)\n",
    "\n",
    "    plt.plot(range(epochs),h['loss'],label='Train')\n",
    "    #plt.plot(range(epochs),h['val_loss'],label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [06:22<00:00, 22.49s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "h5f = h5py.File('/odinn/tmp/benediktj/Data/SplicePrediction/gencode_40k_dataset_test_.h5')\n",
    "\n",
    "num_idx = len(h5f.keys())//2\n",
    "\n",
    "test_dataset = h5pyDataset(h5f,list(range(num_idx)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models)]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "ce_2d = []\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks[0].to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks[0].to(device),0),1,2)\n",
    "    #print(np.max(target_chunks.cpu().numpy()[:,2,:]))\n",
    "    n_chunks = int(np.ceil(batch_chunks.shape[0]/BATCH_SIZE))\n",
    "    batch_chunks = torch.chunk(batch_chunks, n_chunks, dim=0)\n",
    "    target_chunks = torch.chunk(target_chunks, n_chunks, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        outputs = torch.mean(torch.stack(outputs),dim=0)\n",
    "        #outputs = odds_gmean(torch.stack(outputs))\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "        targets_list.extend(targets.unsqueeze(0))\n",
    "        outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.0002340018349389342\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9952\t0.9491\t0.9879\t0.9912\t0.9769\t0.9794\t0.3031\t0.0018\t0.0003\t13561\t14289.0\t14289\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9958\t0.9519\t0.9896\t0.9925\t0.9799\t0.9790\t0.3300\t0.0016\t0.0002\t13601\t14289.0\t14289\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'Y_true_acceptor':Y_true_acceptor, 'Y_pred_acceptor':Y_pred_acceptor,'Y_true_donor':Y_true_donor, 'Y_pred_donor':Y_pred_donor}).to_csv('/odinn/tmp/benediktj/Data/gencode_transformer_splice_site_pred.csv.gz',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.00022536547672551296\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9964\t0.9501\t0.9878\t0.9917\t0.9781\t0.9797\t0.3242\t0.0025\t0.0003\t13576\t14289.0\t14289\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9959\t0.9535\t0.9903\t0.9926\t0.9808\t0.9799\t0.3404\t0.0020\t0.0002\t13624\t14289.0\t14289\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [16:52<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision score = 0.9794794917106628 (95% confidence interval: [0.9781289353966713, 0.9808172889053821])\n",
      "topk score = 0.9517810903492196 (95% confidence interval: [0.9500151536188121, 0.9534903753337829])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_bootstrap(Y_true_acceptor,Y_pred_acceptor,Y_true_donor,Y_pred_donor,n_bootstraps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data'\n",
    "setType = 'test'\n",
    "annotation_test, transcriptToLabel_test, seqData = getData(data_dir, setType)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1386/1386 [2:00:38<00:00,  5.22s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models)]\n",
    "#nr = [0,2,3]\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_201221_{}'.format(nr[i]))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListFull(annotation_test,transcriptToLabel_test,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=32, pin_memory=True)\n",
    "\n",
    "\n",
    "#targets_list = []\n",
    "#outputs_list = []\n",
    "ce_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = torch.mean(outputs,dim=0)\n",
    "    #outputs = odds_gmean(outputs)\n",
    "    #targets_list.extend(targets.unsqueeze(0))\n",
    "    #outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.0001054637939021545\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9811\t0.9409\t0.992\t0.996\t0.967\t0.9827\t0.4555\t0.0022\t0.0003\t84407\t89712.0\t89712\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9823\t0.9457\t0.9945\t0.9972\t0.9704\t0.9829\t0.4890\t0.0018\t0.0002\t84841\t89712.0\t89712\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9432851792402355 0.968739926815033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [47:04<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision score = 0.968739926815033 (95% confidence interval: [0.9676340147852898, 0.9693381801247597])\n",
      "topk score = 0.9432851792402355 (95% confidence interval: [0.9423510116579243, 0.9438788249752162])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_bootstrap(df['Y_true_acceptor'].astype(np.int8),df['Y_pred_acceptor'].astype(np.float32),df['Y_true_donor'].astype(np.int8),df['Y_pred_donor'].astype(np.float32),n_bootstraps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.00022591106861107486\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9961\t0.95\t0.9876\t0.9914\t0.9782\t0.9778\t0.3168\t0.0024\t0.0004\t13574\t14289.0\t14289\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9961\t0.9533\t0.9899\t0.9926\t0.9807\t0.9781\t0.3336\t0.0020\t0.0003\t13621\t14289.0\t14289\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1386/1386 [1:27:43<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models[:5])]\n",
    "#nr = [0,2,3]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_161022_{}'.format(i))) for i,model in enumerate(models[5:])]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListFull(annotation_test,transcriptToLabel_test,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "#targets_list = []\n",
    "#outputs_list = []\n",
    "ce_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = torch.mean(outputs,dim=0)\n",
    "    #outputs = odds_gmean(outputs)\n",
    "    #targets_list.extend(targets.unsqueeze(0))\n",
    "    #outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.00010540342260087413\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9813\t0.941\t0.9921\t0.9959\t0.9673\t0.9811\t0.4463\t0.0022\t0.0003\t84422\t89712.0\t89712\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9825\t0.946\t0.9944\t0.9971\t0.9711\t0.9814\t0.4852\t0.0018\t0.0002\t84871\t89712.0\t89712\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Y_true_acceptor':Y_true_acceptor,'Y_pred_acceptor':Y_pred_acceptor,'Y_true_donor':Y_true_donor,'Y_pred_donor':Y_pred_donor})\n",
    "#df.to_csv('/odinn/tmp/benediktj/Data/SplicePrediction-050422/transformer_40k_test_set_predictions_191022.gz',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/odinn/tmp/benediktj/Data/SplicePrediction-050422/transformer_40k_test_set_predictions_191022.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9435359818084537 0.9691827595233917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [47:05<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision score = 0.9691827595233917 (95% confidence interval: [0.9681409023702144, 0.9697520196437835])\n",
      "topk score = 0.9435359818084537 (95% confidence interval: [0.9425768501761581, 0.944104919183477])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_bootstrap(df['Y_true_acceptor'].astype(np.int8),df['Y_pred_acceptor'].astype(np.float32),df['Y_true_donor'].astype(np.int8),df['Y_pred_donor'].astype(np.float32),n_bootstraps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [04:55<00:00, 17.35s/it]\n"
     ]
    }
   ],
   "source": [
    "def odds_gmean(prediction,n_models,a = 1.5):\n",
    "    p = torch.pow(torch.prod(prediction,dim=0), a/n_models)\n",
    "    p_neg = torch.pow(torch.prod(1-prediction,dim=0), a/n_models)\n",
    "    return p / (p+p_neg)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "h5f = h5py.File('/odinn/tmp/benediktj/Data/SplicePrediction/gencode_40k_dataset_test_.h5')\n",
    "\n",
    "num_idx = len(h5f.keys())//2\n",
    "\n",
    "test_dataset = h5pyDataset(h5f,list(range(num_idx)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models[:5])]\n",
    "#nr = [0,2,3]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_161022_{}'.format(i))) for i,model in enumerate(models[5:])]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "ce_2d = []\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks[0].to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks[0].to(device),0),1,2)\n",
    "    #print(np.max(target_chunks.cpu().numpy()[:,2,:]))\n",
    "    n_chunks = int(np.ceil(batch_chunks.shape[0]/BATCH_SIZE))\n",
    "    batch_chunks = torch.chunk(batch_chunks, n_chunks, dim=0)\n",
    "    target_chunks = torch.chunk(target_chunks, n_chunks, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        #outputs = torch.mean(torch.stack(outputs),dim=0)\n",
    "        outputs = odds_gmean(torch.stack(outputs),n_models)\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "        targets_list.extend(targets.unsqueeze(0))\n",
    "        outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.00028597514309361134\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9955\t0.95\t0.9875\t0.9909\t0.9773\t0.9980\t0.1311\t0.0000\t0.0000\t13575\t14289.0\t14289\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9961\t0.9532\t0.9899\t0.9923\t0.9803\t0.9980\t0.1549\t0.0000\t0.0000\t13620\t14289.0\t14289\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1386/1386 [1:23:22<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models[:5])]\n",
    "#nr = [0,2,3]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_161022_{}'.format(i))) for i,model in enumerate(models[5:])]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListFull(annotation_test,transcriptToLabel_test,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "#targets_list = []\n",
    "#outputs_list = []\n",
    "ce_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "    outputs = torch.stack(outputs)\n",
    "    #outputs = torch.mean(outputs,dim=0)\n",
    "    outputs = odds_gmean(outputs,n_models)\n",
    "    #targets_list.extend(targets.unsqueeze(0))\n",
    "    #outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.00014547907163697655\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9813\t0.941\t0.9918\t0.9955\t0.9671\t0.9984\t0.3592\t0.0000\t0.0000\t84418\t89712.0\t89712\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9823\t0.9459\t0.9941\t0.9969\t0.9707\t0.9984\t0.4408\t0.0000\t0.0000\t84857\t89712.0\t89712\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Y_true_acceptor':Y_true_acceptor,'Y_pred_acceptor':Y_pred_acceptor,'Y_true_donor':Y_true_donor,'Y_pred_donor':Y_pred_donor})\n",
    "#df.to_csv('/odinn/tmp/benediktj/Data/SplicePrediction-050422/transformer_40k_test_set_predictions_191022_2.gz',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader import get_GTEX_v8_Data,getDataPointListGTEX\n",
    "setType = 'test'\n",
    "annotation_test, gene_to_label, seqData = get_GTEX_v8_Data('/odinn/tmp/benediktj/Data/SplicePrediction-GTEX-V8', setType,'annotation_GTEX_v8.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [47:38<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListGTEX(annotation_test,gene_to_label,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "ce_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = torch.mean(outputs,dim=0)\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.0015977755472590298\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9982\t0.7133\t0.8128\t0.8735\t0.7789\t0.8781\t0.0037\t0.0004\t0.0001\t63915\t89600.0\t89600\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.998\t0.7093\t0.8043\t0.8585\t0.7648\t0.8771\t0.0027\t0.0003\t0.0001\t64736\t91272.0\t91272\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor>0, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor>0, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7113008346128683 0.7718501091003418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [21:20<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision score = 0.7718501091003418 (95% confidence interval: [0.7704519890248775, 0.7741622895002365])\n",
      "topk score = 0.7113008346128683 (95% confidence interval: [0.7099157601169932, 0.7134090427928569])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "run_bootstrap(Y_true_acceptor,Y_pred_acceptor,Y_true_donor,Y_pred_donor,n_bootstraps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader import get_GTEX_v8_Data,getDataPointListGTEX\n",
    "data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction-rnasplice-blood-070623/'\n",
    "setType = 'test'\n",
    "annotation_test, gene_to_label, seqData = get_GTEX_v8_Data(data_dir, setType,'annotation_GTEX_v8.txt')\n",
    "from src.dataloader import getDataPointListFull,DataPointFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16*k*N_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 803/803 [52:56<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True,crop=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models)]\n",
    "\n",
    "\n",
    "#for i,model in enumerate(models):\n",
    "#        state_dict = torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))\n",
    "#        new_state_dict = OrderedDict()\n",
    "#        for k, v in state_dict.items():\n",
    "#            name = k[7:] # remove `module.`\n",
    "#            new_state_dict[name] = v\n",
    "#        model.load_state_dict(new_state_dict)\n",
    "\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_finetune_rnasplice-blood_all_050623_{}'.format(i))) for i,model in enumerate(models)]\n",
    "#nr = [0,2,3]\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_201221_{}'.format(nr[i]))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListGTEX(annotation_test,gene_to_label,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "ce_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = torch.mean(outputs,dim=0)\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.0018605105671323805\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9972\t0.6902\t0.7941\t0.8592\t0.7543\t0.7553\t0.0029\t0.0004\t0.0001\t68242\t98870.0\t98870\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9972\t0.6927\t0.7909\t0.8457\t0.746\t0.7619\t0.0022\t0.0003\t0.0001\t69353\t100114.0\t100114\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor>0, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor>0, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6914951123818371 0.7501385509967804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [24:33<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision score = 0.7501385509967804 (95% confidence interval: [0.748635719716549, 0.7519790798425674])\n",
      "topk score = 0.6914951123818371 (95% confidence interval: [0.6899768139575603, 0.6933603400151905])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "run_bootstrap(Y_true_acceptor,Y_pred_acceptor,Y_true_donor,Y_pred_donor,n_bootstraps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader import get_GTEX_Data,getDataPointListGTEX,DataPointGTEX\n",
    "from src.evaluation_metrics import kullback_leibler_divergence_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setType = 'test'\n",
    "annotation_test, gene_to_label, seqData = get_GTEX_Data('/odinn/tmp/benediktj/Data/SplicePrediction-GTEX', setType)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 892/892 [56:03<00:00,  3.77s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models[:5])]\n",
    "#nr = [0,2,3]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_161022_{}'.format(i))) for i,model in enumerate(models[5:])]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListGTEX(annotation_test,gene_to_label,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "#targets_list = []\n",
    "#outputs_list = []\n",
    "kl_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = torch.mean(outputs,dim=0)\n",
    "    #outputs = odds_gmean(outputs)\n",
    "    #targets_list.extend(targets.unsqueeze(0))\n",
    "    #outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    kl_2d.append(kullback_leibler_divergence_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Div = 0.00016740387218513566\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9801\t0.9247\t0.9749\t0.983\t0.9478\t0.9811\t0.3257\t0.0023\t0.0003\t50826\t54965.0\t54965\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9724\t0.9186\t0.9729\t0.9827\t0.9368\t0.9814\t0.3591\t0.0019\t0.0002\t50474\t54944.0\t54944\n"
     ]
    }
   ],
   "source": [
    "mean_kl = np.mean(kl_2d)\n",
    "print('KL Div = {}'.format(mean_kl))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(np.round(Y_true_acceptor,0), Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(np.round(Y_true_donor,0), Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9854\t0.8814\t0.9382\t0.9592\t0.9153\t0.9769\t0.1012\t0.0016\t0.0003\t53825\t61069.0\t61069\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9811\t0.8695\t0.932\t0.9565\t0.9031\t0.9767\t0.0830\t0.0012\t0.0002\t53998\t62103.0\t62103\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor>0, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor>0, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [05:01<00:00, 17.73s/it]\n"
     ]
    }
   ],
   "source": [
    "def odds_gmean(prediction,n_models,a = 1.5):\n",
    "    p = torch.pow(torch.prod(prediction,dim=0), a/n_models)\n",
    "    p_neg = torch.pow(torch.prod(1-prediction,dim=0), a/n_models)\n",
    "    return p / (p+p_neg)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "h5f = h5py.File('/odinn/tmp/benediktj/Data/SplicePrediction/gencode_40k_dataset_test_.h5')\n",
    "\n",
    "num_idx = len(h5f.keys())//2\n",
    "\n",
    "test_dataset = h5pyDataset(h5f,list(range(num_idx)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models)]\n",
    "#nr = [0,2,3]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "ce_2d = []\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks[0].to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks[0].to(device),0),1,2)\n",
    "    #print(np.max(target_chunks.cpu().numpy()[:,2,:]))\n",
    "    n_chunks = int(np.ceil(batch_chunks.shape[0]/BATCH_SIZE))\n",
    "    batch_chunks = torch.chunk(batch_chunks, n_chunks, dim=0)\n",
    "    target_chunks = torch.chunk(target_chunks, n_chunks, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        outputs = torch.mean(torch.stack(outputs),dim=0)\n",
    "        #outputs = odds_gmean(torch.stack(outputs),n_models)\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "        targets_list.extend(targets.unsqueeze(0))\n",
    "        outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.00022536547672551296\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9964\t0.9501\t0.9878\t0.9917\t0.9781\t0.9797\t0.3242\t0.0025\t0.0003\t13576\t14289.0\t14289\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9959\t0.9535\t0.9903\t0.9926\t0.9808\t0.9799\t0.3404\t0.0020\t0.0002\t13624\t14289.0\t14289\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "setType = 'test'\n",
    "annotation_test, transcriptToLabel_test, seqData = getData(data_dir, setType)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1386/1386 [1:22:25<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_171022_{}'.format(i))) for i,model in enumerate(models)]\n",
    "#nr = [0,2,3]\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_201221_{}'.format(nr[i]))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(getDataPointListFull(annotation_test,transcriptToLabel_test,SL,CL_max,shift=SL))\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "#targets_list = []\n",
    "#outputs_list = []\n",
    "ce_2d = []\n",
    "for (batch_features ,targets) in tqdm(test_loader):\n",
    "    batch_features = batch_features.type(torch.FloatTensor).to(device)\n",
    "    targets = targets.to(device)[:,:,CL_max//2:-CL_max//2]\n",
    "    outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "    #outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "    outputs = torch.stack(outputs)\n",
    "    outputs = torch.mean(outputs,dim=0)\n",
    "    #outputs = odds_gmean(outputs)\n",
    "    #targets_list.extend(targets.unsqueeze(0))\n",
    "    #outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(targets,1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(outputs,1,2).cpu().numpy()\n",
    "    ce_2d.append(cross_entropy_2d(targets,outputs))\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy = 0.0001054637939021545\n",
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9811\t0.9409\t0.992\t0.996\t0.967\t0.9827\t0.4555\t0.0022\t0.0003\t84407\t89712.0\t89712\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9823\t0.9457\t0.9945\t0.9972\t0.9704\t0.9829\t0.4890\t0.0018\t0.0002\t84841\t89712.0\t89712\n"
     ]
    }
   ],
   "source": [
    "mean_ce = np.mean(ce_2d)\n",
    "print('Cross entropy = {}'.format(mean_ce))\n",
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Y_true_acceptor':Y_true_acceptor,'Y_pred_acceptor':Y_pred_acceptor,'Y_true_donor':Y_true_donor,'Y_pred_donor':Y_pred_donor})\n",
    "df.to_csv('/odinn/tmp/benediktj/Data/SplicePrediction-050422/transformer_40k_test_set_predictions_261022.gz',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
