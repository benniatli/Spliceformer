{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from math import ceil\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import pickle5 as pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from glob import glob\n",
    "\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(23673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gffutils\n",
    "#gtf = gffutils.FeatureDB(\"/odinn/tmp/benediktj/Data/SplicePrediction/Homo_sapiens.GRCh38.87.db\")\n",
    "#transcriptToLSupport = {}\n",
    "#setType = 'train'\n",
    "#for transcript_id in tqdm(annotation['transcript']):\n",
    "#    transcript = gtf[transcript_id.split('.')[0]]\n",
    "#    transcriptToLSupport[transcript_id] = int(transcript[\"transcript_support_level\"][0])\n",
    "#gtf = gffutils.FeatureDB(\"/odinn/tmp/benediktj/Data/SplicePrediction/Homo_sapiens.GRCh38.87.db\")\n",
    "#genes = gtf.features_of_type('gene')\n",
    "#geneStartEnd = {}\n",
    "#for gene in tqdm(genes):\n",
    "#    if gene['gene_biotype'][0]=='protein_coding':\n",
    "#        geneStartEnd['{}.{}'.format(gene['gene_id'][0],gene['gene_version'][0])] = [gene[3],gene[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction'\n",
    "#with open(data_dir+'geneStartEnd.pickle', 'wb') as handle:\n",
    "#    pickle.dump(geneStartEnd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#with open(data_dir+'/transcriptSupport.pickle', 'wb') as handle:\n",
    "#    pickle.dump(transcriptToLSupport, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction-050422/'\n",
    "#with open(data_dir+'transcriptSupport.pickle', 'rb') as handle:\n",
    "#    transcriptToSupport = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gtf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Model\n",
    "###############################################################################\n",
    "\n",
    "L = 32\n",
    "N_GPUS = 8\n",
    "\n",
    "#if int(sys.argv[1]) == 80:\n",
    "#    W = np.asarray([11, 11, 11, 11])\n",
    "#    AR = np.asarray([1, 1, 1, 1])\n",
    "#    BATCH_SIZE = 18*N_GPUS\n",
    "#elif int(sys.argv[1]) == 400:\n",
    "#    W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11])\n",
    "#    AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4])\n",
    "#    BATCH_SIZE = 18*N_GPUS\n",
    "#elif int(sys.argv[1]) == 2000:\n",
    "#    W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11,\n",
    "#                    21, 21, 21, 21])\n",
    "#    AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4,\n",
    "#                     10, 10, 10, 10])\n",
    "#    BATCH_SIZE = 12*N_GPUS\n",
    "#elif int(sys.argv[1]) == 10000:\n",
    "W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11,\n",
    "                21, 21, 21, 21, 41, 41, 41, 41])\n",
    "AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4,\n",
    "                 10, 10, 10, 10, 25, 25, 25, 25])\n",
    "BATCH_SIZE = 6*N_GPUS\n",
    "# Hyper-parameters:\n",
    "# L: Number of convolution kernels\n",
    "# W: Convolution window size in each residual unit\n",
    "# AR: Atrous rate in each residual unit\n",
    "\n",
    "CL = 2 * np.sum(AR*(W-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction-050422'\n",
    "setType = 'train'\n",
    "#with open('{}/sparse_sequence_data_{}.pickle'.format(data_dir,setType), 'rb') as handle:\n",
    "#    seqData = pickle.load(handle)\n",
    "    \n",
    "with open('{}/sparse_discrete_label_data_{}.pickle'.format(data_dir,setType), 'rb') as handle:\n",
    "    transcriptToLabel = pickle.load(handle)\n",
    "    \n",
    "CHROM_TRAIN = ['chr11', 'chr13', 'chr15', 'chr17', 'chr19', 'chr21',\n",
    "                           'chr2', 'chr4', 'chr6', 'chr8', 'chr10', 'chr12',\n",
    "                           'chr14', 'chr16', 'chr18', 'chr20', 'chr22', 'chrX', 'chrY']\n",
    "annotation = pd.read_csv(data_dir+'/annotation_ensembl_v87_train.txt',sep='\\t',header=None)[[0,1,2,3,4]]\n",
    "annotation.columns = ['name','chrom','strand','tx_start','tx_end']\n",
    "annotation['transcript'] = annotation['name'].apply(lambda x: x.split('---')[-2].split('.')[0]).values\n",
    "annotation['gene'] = annotation['name'].apply(lambda x: x.split('---')[-3].split('.')[0]).values\n",
    "#annotation['support'] = annotation['transcript'].apply(lambda x:transcriptToSupport[x])\n",
    "\n",
    "chrom_paths = glob(data_dir+'/sparse_sequence_data/*')\n",
    "chromToPath = {}\n",
    "for path in chrom_paths:\n",
    "    chromToPath[path.split('/')[-1].split('_')[0]] = path\n",
    "    \n",
    "seqData = {}\n",
    "for chrom in CHROM_TRAIN:\n",
    "    seqData[chrom] = load_npz(data_dir+'/sparse_sequence_data/{}_train.npz'.format(chrom)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_max=40000\n",
    "# Maximum nucleotide context length (CL_max/2 on either side of the \n",
    "# position of interest)\n",
    "# CL_max should be an even number\n",
    "\n",
    "SL=5000\n",
    "# Sequence length of SpliceAIs (SL+CL will be the input length and\n",
    "# SL will be the output length)\n",
    "\n",
    "#splice_table='/odinn/tmp/benediktj/Data/SplicePrediction/annotation_ensembl_v87_train.txt'\n",
    "#ref_genome='/odinn/tmp/benediktj/SpliceAITrainingCode/genome.fa'\n",
    "# Input details\n",
    "\n",
    "#data_dir='/odinn/tmp/benediktj/Data/SplicePrediction/'\n",
    "#sequence='/odinn/tmp/benediktj/SpliceAITrainingCode/canonical_sequence.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert CL_max % 2 == 0\n",
    "\n",
    "def print_topl_statistics(y_true, y_pred):\n",
    "    # Prints the following information: top-kL statistics for k=0.5,1,2,4,\n",
    "    # auprc, thresholds for k=0.5,1,2,4, number of true splice sites.\n",
    "\n",
    "    idx_true = np.nonzero(y_true == 1)[0]\n",
    "    argsorted_y_pred = np.argsort(y_pred)\n",
    "    sorted_y_pred = np.sort(y_pred)\n",
    "\n",
    "    topkl_accuracy = []\n",
    "    threshold = []\n",
    "\n",
    "    for top_length in [0.5, 1, 2, 4]:\n",
    "\n",
    "        idx_pred = argsorted_y_pred[-int(top_length*len(idx_true)):]\n",
    "\n",
    "        topkl_accuracy += [np.size(np.intersect1d(idx_true, idx_pred)) \\\n",
    "                  / float(min(len(idx_pred), len(idx_true)))]\n",
    "        threshold += [sorted_y_pred[-int(top_length*len(idx_true))]]\n",
    "\n",
    "    auprc = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "          np.round(topkl_accuracy[0],4), np.round(topkl_accuracy[1],4), np.round(topkl_accuracy[2],4),\n",
    "          np.round(topkl_accuracy[3],4), np.round(auprc,4), np.round(threshold[0],4), np.round(threshold[1],4),\n",
    "          np.round(threshold[2],4), np.round(threshold[3],4), len(idx_true)))\n",
    "    return (topkl_accuracy,[auprc],threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        \n",
    "        self.gate = nn.Linear(dim, inner_dim)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        \n",
    "        \n",
    "        \n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        #indices = torch.triu_indices(dots.shape[2], dots.shape[3], offset=1)\n",
    "        #dots[:,:, indices[0], indices[1]] = float('-inf')\n",
    "        \n",
    "        attn = self.attend(dots)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = torch.sigmoid(self.gate(x))*out\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size,dilation=1, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        paddingAmount = int(dilation*(kernel_size-1)/2)\n",
    "        self.convlayer1 = nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size= kernel_size,dilation=dilation,stride=1,padding=paddingAmount,padding_mode='zeros')\n",
    "        self.convlayer2 = nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size= kernel_size,dilation=dilation,stride=1,padding=paddingAmount,padding_mode='zeros')\n",
    "        self.activate = activation_func(activation)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channels, momentum=0.01)\n",
    "        self.bn2 = nn.BatchNorm1d(self.in_channels, momentum=0.01)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        residual = self.shortcut(x)\n",
    "        x = self.activate(self.bn1(x))\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.activate(self.bn2(x))\n",
    "        x = self.convlayer2(x)\n",
    "        x += residual\n",
    "        return x\n",
    "    \n",
    "class ResComboBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,res_W,res_dilation):\n",
    "        super().__init__()\n",
    "        self.comboBlock = nn.Sequential(\n",
    "        ResidualBlock(in_channels,out_channels,res_W,res_dilation),\n",
    "        ResidualBlock(in_channels,out_channels,res_W,res_dilation),\n",
    "        ResidualBlock(in_channels,out_channels,res_W,res_dilation),\n",
    "        ResidualBlock(in_channels,out_channels,res_W,res_dilation)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.comboBlock(x)\n",
    "    \n",
    "class smallModel(nn.Module):\n",
    "    def __init__(self, temp=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_channels = 32\n",
    "        self.res_W = [11,11,21,41]\n",
    "        res_dilation = [1,4,10,25]\n",
    "        self.kernel_size = 1\n",
    "        self.temp=temp\n",
    "        self.conv_layer_1 = nn.Conv1d(in_channels=5, out_channels=self.n_channels, kernel_size= self.kernel_size,stride=1)\n",
    "        self.skip_layers = nn.ModuleList([nn.Conv1d(in_channels=self.n_channels, out_channels=self.n_channels, kernel_size= self.kernel_size,stride=1) for i in range(5)])\n",
    "        self.res_layers = nn.ModuleList([ResComboBlock(in_channels=self.n_channels, out_channels=self.n_channels, res_W=self.res_W[i], res_dilation=res_dilation[i]) for i in range(4)])\n",
    "        self.conv_final = nn.Conv1d(in_channels=self.n_channels, out_channels=3, kernel_size= self.kernel_size,stride=1)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        x = self.conv_layer_1(features)\n",
    "        skip = self.skip_layers[0](x)\n",
    "\n",
    "        for i,residualUnit in enumerate(self.res_layers):\n",
    "            x = residualUnit(x)\n",
    "            #if i ==2:\n",
    "            #    x_2 = x\n",
    "            skip += self.skip_layers[i+1](x)\n",
    "            #skip = torch.cat([skip,self.skip_layers[i+1](x)],axis=1)\n",
    "        \n",
    "        x_skip = skip[:,:,:]\n",
    "        x_cropped = skip[:,:,CL_max//2:-CL_max//2]\n",
    "        #x = self.conv_final(x_cropped)\n",
    "        #m_1 = nn.Softmax(dim=1)\n",
    "        #out_1 = m_1(x/temp)\n",
    "        #attention = m_1(self.conv_final(x_skip)/temp)\n",
    "        return x_cropped, x_skip\n",
    "    \n",
    "class SpliceFormerBlock(nn.Module):\n",
    "    def __init__(self, temp=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_channels = 32\n",
    "        \n",
    "        self.maxSeqLength = 4*128\n",
    "        \n",
    "        #self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=32, dim_feedforward=128*4, nhead=8,norm_first=True), num_layers=12)\n",
    "        self.transformer =  Transformer(self.n_channels, depth=4, heads=4, dim_head=32, mlp_dim=self.n_channels*16, dropout=0)\n",
    "        \n",
    "    def forward(self, x_skip,attention,pos_emb):\n",
    "        x_emb1 = torch.transpose(x_skip, 1, 2)\n",
    "        x_emb = x_emb1 +  pos_emb\n",
    "        acceptors_sorted = torch.argsort(attention[:,1,:],dim=1,descending=True)[:,:self.maxSeqLength//2:]\n",
    "        donors_sorted = torch.argsort(attention[:,2,:],dim=1,descending=True)[:,:self.maxSeqLength//2]\n",
    "        splice_idx = torch.cat([acceptors_sorted,donors_sorted],dim=1)\n",
    "        splice_idx = torch.sort(splice_idx,dim=1).values\n",
    "        splice_idx = splice_idx.unsqueeze(2).repeat(1,1,self.n_channels)\n",
    "        embedding = torch.gather(x_emb,1,splice_idx)\n",
    "        embedding = self.transformer(embedding)\n",
    "        tmp = torch.zeros_like(x_emb1)\n",
    "        embedding = tmp.scatter_(1, splice_idx, embedding) \n",
    "        embedding = torch.transpose(embedding, 1, 2)\n",
    "        #embedding = x_skip+embedding\n",
    "        return embedding\n",
    "    \n",
    "class SpliceFormer(nn.Module):\n",
    "    def __init__(self, temp=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_channels = 32\n",
    "        self.res_W = [11,11,21,41]\n",
    "        res_dilation = [1,4,10,25]\n",
    "        self.kernel_size = 1\n",
    "        self.smallModel = smallModel(temp=temp).apply(keras_init)\n",
    "        #self.res_kernel_size = 11\n",
    "        self.temp=temp\n",
    "        self.skip_layers = nn.ModuleList([nn.Conv1d(in_channels=self.n_channels, out_channels=self.n_channels, kernel_size= self.kernel_size,stride=1) for i in range(2)])\n",
    "        self.res_layers = nn.ModuleList([ResComboBlock(in_channels=self.n_channels, out_channels=self.n_channels, res_W=self.res_W[i], res_dilation=res_dilation[i]) for i in range(1)])\n",
    "        self.conv_final = nn.Conv1d(in_channels=self.n_channels, out_channels=3, kernel_size= self.kernel_size,stride=1)\n",
    "        self.pos_emb = FixedPositionalEmbedding(self.n_channels)\n",
    "        self.transformerBlock1 = SpliceFormerBlock()\n",
    "        self.transformerBlock2 = SpliceFormerBlock()\n",
    "        #self.transformerBlock3 = SpliceFormerBlock()\n",
    "        #self.transformerBlock4 = SpliceFormerBlock()\n",
    "        #self.transformerBlock5 = SpliceFormerBlock()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, features):\n",
    "        out1,x_skip = self.smallModel(features)\n",
    "        m_1 = nn.Softmax(dim=1)\n",
    "        out1 = m_1(self.conv_final(out1))\n",
    "        attention =  m_1(self.conv_final(x_skip))\n",
    "        pos_emb = self.pos_emb(torch.transpose(x_skip, 1, 2)).type_as(torch.transpose(x_skip, 1, 2))\n",
    "        emb1 = self.transformerBlock1(x_skip,attention,pos_emb)\n",
    "        \n",
    "        tmp = x_skip+emb1\n",
    "        attention =  m_1(self.conv_final(tmp)) \n",
    "        emb2 = self.transformerBlock2(tmp,attention,pos_emb)\n",
    "        \n",
    "        #tmp = x_skip+emb1+emb2\n",
    "        #attention =  m_1(self.conv_final(tmp))\n",
    "        #emb3 = self.transformerBlock3(tmp,attention,pos_emb)\n",
    "        #attention =  m_1(self.conv_final(x_skip))\n",
    "        \n",
    "        #tmp = x_skip+emb1+emb2+emb3\n",
    "        #attention =  m_1(self.conv_final(tmp))\n",
    "        #emb4 = self.transformerBlock4(tmp,attention,pos_emb)\n",
    "        \n",
    "        #tmp = x_skip+emb1+emb2+emb3+emb4\n",
    "        #attention =  m_1(self.conv_final(tmp))\n",
    "        #emb5 = self.transformerBlock5(tmp,attention,pos_emb)\n",
    "        \n",
    "        x_skip = x_skip+emb1+emb2\n",
    "        #x_skip = x_skip+emb1+emb2+emb3+emb4\n",
    "        m_2 = nn.Softmax(dim=1)\n",
    "        out_2 = m_2(self.conv_final(x_skip))\n",
    "        out_2 = out_2[:,:,(CL_max//2):-(CL_max//2)]\n",
    "        #print(out_2.shape)\n",
    "        #out_2 = torch.zeros_like(out_1)\n",
    "        #out_2 = m_2(x_skip+x_emb)\n",
    "        return out1,out_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene, validation_gene = train_test_split(annotation['gene'].drop_duplicates(),test_size=.1,random_state=435)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotation = annotation.drop_duplicates('gene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_train = annotation[annotation['gene'].isin(train_gene)]\n",
    "annotation_validation = annotation[annotation['gene'].isin(validation_gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_MAP = np.asarray([[1, 0, 0],\n",
    "                      [0, 1, 0],\n",
    "                      [0, 0, 1],\n",
    "                      [0, 0, 0]])\n",
    "\n",
    "class spliceDataset(Dataset):\n",
    "    def __init__(self, annotation, transform=None, target_transform=None):\n",
    "        #self.data = data\n",
    "        #self.labelDict = labelDict\n",
    "        self.annotation = annotation\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.annotation.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transcript,chrom,strand,tx_start,tx_end = self.annotation['transcript'].values[idx],self.annotation['chrom'].values[idx],self.annotation['strand'].values[idx],self.annotation['tx_start'].values[idx],self.annotation['tx_end'].values[idx]\n",
    "        #gene,chrom,strand = self.annotation['gene'].values[idx],self.annotation['chrom'].values[idx],self.annotation['strand'].values[idx]\n",
    "        \n",
    "        length = tx_end-tx_start\n",
    "        num_points = ceil_div(length, SL)\n",
    "        \n",
    "        Xd = np.zeros((num_points, SL+CL_max,5))\n",
    "        Yd = np.zeros((num_points, SL,3))\n",
    "        if strand=='+':\n",
    "            X0 = seqData[chrom][int(tx_start)-1-CL_max//2:int(tx_end)+SL+CL_max//2].toarray()\n",
    "            X0[:CL_max//2,:] = np.array([0,0,0,0,0])\n",
    "            X0[-(CL_max//2+SL):,:] = np.array([0,0,0,0,0])\n",
    "        else:\n",
    "            X0 = seqData[chrom][int(tx_start)-1-SL-CL_max//2:int(tx_end)+CL_max//2].toarray()\n",
    "            X0[:(CL_max//2+SL),:] = np.array([0,0,0,0,0])\n",
    "            X0[-CL_max//2:,:] = np.array([0,0,0,0,0])\n",
    "            X0 = X0[::-1,:]\n",
    "            X0[:,[0,1,2,3]] = X0[:,[0,1,2,3]][:,::-1]\n",
    "            \n",
    "        label = transcriptToLabel[transcript]\n",
    "        Y0 = np.zeros((length+SL,3))\n",
    "        Y0[:length,0] = np.ones(length)\n",
    "        Y0[label[1],:] = OUT_MAP[np.array(label[0]).astype('int8')]\n",
    "\n",
    "        for i in range(num_points):\n",
    "            tmp = X0[SL*i:CL_max+SL*(i+1),:]\n",
    "            if tmp.shape[0]==0:\n",
    "                continue\n",
    "            Xd[i,:,:] = tmp\n",
    "            Yd[i,:,:] = Y0[SL*i:SL*(i+1),:]\n",
    "        \n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        \n",
    "        #X = torch.Tensor(X.copy())\n",
    "        #print(X.shape)\n",
    "        #X = torch.nn.functional.pad(X.clone(), (0,0,CL_max//2,CL_max//2+X.shape[0]-SL*(X.shape[0]//SL)), \"constant\", 0)\n",
    "        #X = torch.transpose(X.unfold(0,SL,CL_max//2),1,2)\n",
    "        #print(X.shape)\n",
    "        #Y = torch.transpose(torch.Tensor(Y).unfold(0,SL,CL_max//2),1,2)\n",
    "        return Xd,Yd\n",
    "        #if self.include_prob:\n",
    "        #    return X, [Y[t] for t in range(1)],[Y_prob[t] for t in range(1)]\n",
    "        #else:\n",
    "        #    return X, [Y[t] for t in range(1)]\n",
    "\n",
    "def ceil_div(x, y):\n",
    "    return int(ceil(float(x)/y))\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (example, label)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and label/length are scalars\n",
    "    \"\"\"\n",
    "    #unfold1 = nn.Unfold((SL*3,1),SL,CL_max//2)\n",
    "    #unfold2 = nn.Unfold((SL,1),SL,0)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        features.append(torch.Tensor(data[i][0]))\n",
    "        labels.append(torch.Tensor(data[i][1]))\n",
    "        #features.append(tmp.unfold(0,SL*3,CL_max//2))\n",
    "        #labels.append(torch.Tensor(data[i][1]).unfold(0,SL,CL_max//2))\n",
    "    return torch.cat(features,dim=0).float(), torch.cat(labels,dim=0).float()\n",
    "\n",
    "train_dataset = spliceDataset(annotation_train)\n",
    "val_dataset = spliceDataset(annotation_validation)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=16,collate_fn=collate_fn, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False,collate_fn=collate_fn, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_topl_statistics(y_true, y_pred):\n",
    "    # Prints the following information: top-kL statistics for k=0.5,1,2,4,\n",
    "    # auprc, thresholds for k=0.5,1,2,4, number of true splice sites.\n",
    "\n",
    "    idx_true = np.nonzero(y_true == 1)[0]\n",
    "    argsorted_y_pred = np.argsort(y_pred)\n",
    "    sorted_y_pred = np.sort(y_pred)\n",
    "\n",
    "    topkl_accuracy = []\n",
    "    threshold = []\n",
    "\n",
    "    for top_length in [0.5, 1, 2, 4]:\n",
    "\n",
    "        idx_pred = argsorted_y_pred[-int(top_length*len(idx_true)):]\n",
    "\n",
    "        topkl_accuracy += [np.size(np.intersect1d(idx_true, idx_pred)) \\\n",
    "                  / float(min(len(idx_pred), len(idx_true)))]\n",
    "        threshold += [sorted_y_pred[-int(top_length*len(idx_true))]]\n",
    "\n",
    "    auprc = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\n",
    "          np.round(topkl_accuracy[0],4), np.round(topkl_accuracy[1],4), np.round(topkl_accuracy[2],4),\n",
    "          np.round(topkl_accuracy[3],4), np.round(auprc,4), np.round(threshold[0],4), np.round(threshold[1],4),\n",
    "          np.round(threshold[2],4), np.round(threshold[3],4), len(idx_true)))\n",
    "    return (topkl_accuracy,[auprc],threshold)\n",
    "\n",
    "def trainModel(model,fileName,criterion,epochs,train_loader,val_loader,alpha=0.5,temp = 1,verbose=1):\n",
    "    learning_rate= 1e-3\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "    \n",
    "    #optimizer_small = torch.optim.Adam(model.module.smallModel.parameters(), lr=learning_rate)\n",
    "    #scheduler_small = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "    warmup = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\n",
    "    losses = {}\n",
    "    losses['train'] = []\n",
    "    losses['val'] = []\n",
    "    val_results = []\n",
    "    best_val = np.inf\n",
    "    dataLoaders = {}\n",
    "    dataLoaders['train'] = train_loader\n",
    "    dataLoaders['val'] = val_loader\n",
    "    multiplier = 0.01\n",
    "    eps = torch.finfo(torch.float32).eps\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for phase in ['train','val']:\n",
    "            loop =tqdm(dataLoaders[phase])\n",
    "            if 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            loss = 0\n",
    "            loss_small = 0\n",
    "            ema_loss = 0\n",
    "            ema_loss_small = 0\n",
    "            ema_l1 = 0\n",
    "            ema_a_recall = 0\n",
    "            ema_d_recall = 0\n",
    "            n_steps_completed = 0\n",
    "            #Y_true_argmax,Y_pred_argmax=[],[]\n",
    "            Y_true_acceptor,Y_true_donor,Y_pred_acceptor,Y_pred_donor=[],[],[],[]\n",
    "            for i,(batch_chunks, target_chunks) in enumerate(loop):\n",
    "                batch_chunks = torch.transpose(batch_chunks.to(device),1,2)\n",
    "                #print(batch_chunks.shape)\n",
    "                target_chunks = torch.transpose(torch.squeeze(target_chunks.to(device),0),1,2)\n",
    "                #prob_chunks = torch.transpose(torch.squeeze(prob_chunks[0].to(device),0),1,2)\n",
    "                #n_chunks = int(np.ceil(batch_chunks.shape[0]/BATCH_SIZE))\n",
    "                #print(batch_chunks.shape)\n",
    "                batch_chunks = torch.split(batch_chunks, BATCH_SIZE, dim=0)\n",
    "\n",
    "                target_chunks = torch.split(target_chunks, BATCH_SIZE, dim=0)\n",
    "                #prob_chunks = torch.split(prob_chunks, BATCH_SIZE, dim=0)\n",
    "                targets_list,outputs_list = [], []\n",
    "                for j in range(len(batch_chunks)):\n",
    "                    batch_features = batch_chunks[j]\n",
    "                    \n",
    "                    #targets = torch.argmax(target_chunks[j],1)\n",
    "                    targets = target_chunks[j]\n",
    "                    #probs = prob_chunks[j]\n",
    "                    if j==0 and phase == 'train':\n",
    "                        initial_X = batch_features\n",
    "                        initial_y = targets\n",
    "                        #initial_y_prob = probs\n",
    "                    if batch_features.shape[0]!=BATCH_SIZE and phase == 'train':\n",
    "                        additional_samples = BATCH_SIZE-batch_features.shape[0]\n",
    "                        batch_features = torch.cat([batch_features,initial_X[:additional_samples,:,:]],axis=0)\n",
    "                        targets = torch.cat([targets,initial_y[:additional_samples,:,:]],axis=0)\n",
    "                        #probs = torch.cat([probs,initial_y_prob[:additional_samples,:,:]],axis=0)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    #optimizer_small.zero_grad()\n",
    "\n",
    "                    out1,out2 = model(batch_features)\n",
    "                    \n",
    "                    if phase == 'val':\n",
    "                        outputs = out2.detach()\n",
    "                        #outputs = torch.zeros_like(out1).scatter_(2, splice_idx[:,:3,:], out2).detach()\n",
    "                        targets_list.extend(target_chunks[j].unsqueeze(0))\n",
    "                        outputs_list.extend(outputs.unsqueeze(0))\n",
    "                    \n",
    "\n",
    "                    #tmp_x = torch.transpose(outputs,1,2).reshape(targets.shape[0]*5000,3)\n",
    "                    #tmp_y = torch.transpose(targets,1,2).reshape(targets.shape[0]*5000,3)\n",
    "                    #keepAxis = tmp_y.sum(axis=1) >= 1\n",
    "                    #tmp_x = tmp_x[keepAxis].unsqueeze(0)\n",
    "                    #tmp_y = tmp_y[keepAxis].unsqueeze(0)\n",
    "                    #train_loss = criterion((tmp_x+eps).log(), tmp_y)\n",
    "                    #train_loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = out2\n",
    "                        #outputs = out1.clone()\n",
    "                        #outputs = outputs.scatter_(2, splice_idx[:,:3,:], out2) \n",
    "                        #meanAcceptor = torch.mean(torch.gather(out1,2,splice_idx[:,:3,:])[:,2,:])\n",
    "                        out_argmax = torch.flatten(torch.argmax(outputs,dim=1))\n",
    "                    #    out1_argmax = torch.flatten(torch.argmax(out1,dim=1))\n",
    "                        target_argmax = torch.flatten(torch.argmax(targets,dim=1))\n",
    "                        a_recall = torch.nanmean((out_argmax[target_argmax==1]==1).type(torch.float32))\n",
    "                        d_recall = torch.nanmean((out_argmax[target_argmax==2]==2).type(torch.float32))\n",
    "                        \n",
    "                    #probs = torch.log(probs+eps)\n",
    "                    #probs = torch.nn.functional.softmax(probs/temp, dim=1)\n",
    "                    train_loss_small = criterion(out1, targets)\n",
    "                    #top_targets = torch.gather(targets,2,splice_idx[:,:3,:])\n",
    "                    #top_out1 = torch.gather(out1,2,splice_idx[:,:3,:])\n",
    "                    l1_dist = (torch.sum(torch.abs(out1-out2))/(BATCH_SIZE*128)).detach()\n",
    "                    #l1_dist = (torch.sum(torch.abs(top_out1-out2))/(BATCH_SIZE*128)).detach()\n",
    "                   # if epoch==0:\n",
    "                   #     train_loss = (train_loss_small +criterion[1](out2,targets))/2\n",
    "                   # else:\n",
    "                    train_loss = criterion(out2,targets)\n",
    "                    #train_loss = criterion[1](torch.gather(out2,2,splice_idx[:,:3,:]), torch.gather(targets,2,splice_idx[:,:3,:]))\n",
    "                    #lambda_1 = 1\n",
    "                    #train_loss = alpha*train_loss1+(1-alpha)*train_loss2\n",
    "                    #train_loss = train_loss1\n",
    "                    if phase == 'train':\n",
    "                        train_loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2.0)\n",
    "                        optimizer.step()\n",
    "                        #if epoch < 5:\n",
    "                        #    train_loss_small.backward()\n",
    "                        #    optimizer_small.step()\n",
    "                        \n",
    "                        if epoch==0:\n",
    "                            warmup.step()\n",
    "                    loss = train_loss.item()+loss\n",
    "                    loss_small = train_loss_small.item()+loss_small\n",
    "                    loop.set_description('Epoch ({}) {}/{}'.format(phase,epoch + 1, epochs))\n",
    "                    n_steps_completed += 1\n",
    "                    if i==0 and j==0:\n",
    "                        ema_loss = train_loss.item()\n",
    "                        ema_loss_small = train_loss_small.item()\n",
    "                        if ~a_recall.isnan():\n",
    "                            ema_a_recall = a_recall.cpu().numpy()\n",
    "                        if ~d_recall.isnan():\n",
    "                            ema_d_recall = a_recall.cpu().numpy()\n",
    "                        #ema_loss_target = train_loss_target.item()\n",
    "                        #ema_loss_probs = train_loss_probs.item()\n",
    "                    else:\n",
    "                        ema_loss = train_loss.item()*multiplier + ema_loss*(1-multiplier)\n",
    "                        ema_loss_small = train_loss_small.item()*multiplier + ema_loss_small*(1-multiplier)\n",
    "                        ema_l1 = l1_dist.cpu().numpy()*multiplier + ema_l1*(1-multiplier)\n",
    "                        if ~a_recall.isnan():\n",
    "                            ema_a_recall = a_recall.cpu().numpy()*multiplier + ema_a_recall*(1-multiplier)\n",
    "                        if ~d_recall.isnan():\n",
    "                            ema_d_recall= d_recall.cpu().numpy()*multiplier + ema_d_recall*(1-multiplier)\n",
    "                        #ema_loss_probs = train_loss_probs.item()*multiplier + ema_loss_probs*(1-multiplier)\n",
    "                    loop.set_postfix(loss=ema_loss,loss_small=ema_loss_small, accepor_recall = ema_a_recall , donor_recall=ema_d_recall,pred_l1_dist=ema_l1)\n",
    "                \n",
    "                if phase == 'val':\n",
    "                    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "                    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "\n",
    "                    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "                    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "                    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "                    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "                    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n",
    "                    #Y_true_argmax.extend(np.argmax(targets[is_expr, :, :],axis=2).flatten())\n",
    "                    #Y_pred_argmax.extend(np.argmax(outputs[is_expr, :, :],axis=2).flatten())\n",
    "            loss = loss / (n_steps_completed)\n",
    "            losses[phase].append(loss)\n",
    "            \n",
    "            if phase == 'val':\n",
    "                Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "                print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "                acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "                print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "                donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)\n",
    "                val_results.append([acceptor_val_results,donor_val_results])\n",
    "                \n",
    "            #if phase == 'val':\n",
    "            #    Y_true_argmax, Y_pred_argmax = np.array(Y_true_argmax), np.array(Y_pred_argmax)\n",
    "                #Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(np.round(Y_true_acceptor,0).astype(int)), np.array(np.round(Y_pred_acceptor,0).astype(int)),np.array(np.round(Y_true_donor,0).astype(int)), np.array(np.round(Y_pred_donor,0).astype(int))\n",
    "            #    print(\"\\n\\033[1m{}:\\033[0m\".format('Total'))\n",
    "            #    print('Accuracy: {}'.format(accuracy_score(Y_true_argmax, Y_pred_argmax)))\n",
    "            #    print('Precision: {}'.format(precision_score(Y_true_argmax, Y_pred_argmax,average='weighted')))\n",
    "            #    print('Recall: {}'.format(recall_score(Y_true_argmax, Y_pred_argmax,average='weighted')))\n",
    "            #    print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "            #    print('Acceptor accuracy: {}'.format(accuracy_score(Y_true_argmax==1, Y_pred_argmax==1)))\n",
    "            #    print('Acceptor precision: {}'.format(precision_score(Y_true_argmax==1, Y_pred_argmax==1)))\n",
    "            #    print('Acceptor recall: {}'.format(recall_score(Y_true_argmax==1, Y_pred_argmax==1)))\n",
    "            #    print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "            #    print('Donor accuracy: {}'.format(accuracy_score(Y_true_argmax==2, Y_pred_argmax==2)))\n",
    "            #    print('Donor precision: {}'.format(precision_score(Y_true_argmax==2, Y_pred_argmax==2)))\n",
    "            #    print('Donor recall: {}'.format(recall_score(Y_true_argmax==2, Y_pred_argmax==2)))\n",
    "                #print('Donor precision: {}'.format(precision_score(Y_true_donor, Y_pred_donor)))\n",
    "                #print('Donor recall: {}'.format(recall_score(Y_true_donor, Y_pred_donor)))\n",
    "                #donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)\n",
    "                #val_results.append([acceptor_val_results,donor_val_results])\n",
    "            \n",
    "            if verbose == 1:\n",
    "                print(\"epoch: {}/{}, {} loss = {:.6f}\".format(epoch + 1, epochs, phase, loss))\n",
    "            if phase == 'val':\n",
    "                torch.save(model.state_dict(), fileName)\n",
    "                #torch.save(smallModel.state_dict(), fileName+\"_small\")\n",
    "                #if loss < best_val:\n",
    "                #    best_val = loss\n",
    "                #    torch.save(model.state_dict(), fileName)\n",
    "        if epoch>5:\n",
    "            scheduler.step()\n",
    "        \n",
    "        #if epoch<5:\n",
    "         #   scheduler_small.step()\n",
    "        \n",
    "    h = pd.DataFrame({'loss':losses['train'],'val_loss':losses['val']})\n",
    "    return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class categorical_crossentropy_2d:\n",
    "    def __init__(self, weights=None,mask=False):\n",
    "        self.weights = weights\n",
    "        self.mask = mask\n",
    "        self.eps = torch.finfo(torch.float32).eps\n",
    "        \n",
    "    def loss(self,y_pred,y_true):\n",
    "        if self.mask:\n",
    "            loss_sum = torch.sum(self.weights[0]*y_true[:, 0, :]*torch.log(y_pred[:, 0, :]+self.eps) + self.weights[1]*y_true[:, 1, :]*torch.log(y_pred[:, 1, :]+self.eps) + self.weights[2]*y_true[:, 2, :]*torch.log(y_pred[:, 2, :]+self.eps))\n",
    "            weight_sum = torch.sum(self.weights[0]*y_true[:, 0, :] + self.weights[1]*y_true[:, 1, :] + self.weights[2]*y_true[:, 2, :])+self.eps\n",
    "            return -loss_sum/weight_sum\n",
    "        else:\n",
    "            prob_sum = torch.sum(y_true)\n",
    "            return -torch.sum(y_true[:, 0, :]*torch.log(y_pred[:, 0, :]+self.eps) + y_true[:, 1, :]*torch.log(y_pred[:, 1, :]+self.eps) + y_true[:, 2, :]*torch.log(y_pred[:, 2, :]+self.eps))/prob_sum\n",
    "        #loss_sum = torch.sum(self.weights[0]*y_true[:, 0, :]*torch.log(y_pred[:, 0, :]+1e-10) + self.weights[1]*y_true[:, 1, :]*torch.log(y_pred[:, 1, :]+1e-10) + self.weights[2]*y_true[:, 2, :]*torch.log(y_pred[:, 2, :]+1e-10))\n",
    "        #weight_sum = torch.sum(self.weights[0]*y_true[:, 0, :] + self.weights[1]*y_true[:, 1, :] + self.weights[2]*y_true[:, 2, :])\n",
    "        #return -loss_sum/weight_sum\n",
    "        \n",
    "class kl_div_2d:\n",
    "    def __init__(self,temp=1):\n",
    "        self.eps = torch.finfo(torch.float32).eps\n",
    "        self.temp = temp\n",
    "        \n",
    "    def loss(self,y_pred,y_true):\n",
    "        if self.temp!=1:\n",
    "            y_true = torch.nn.Softmax(dim=1)(torch.log(y_true+self.eps)/self.temp)\n",
    "        return -torch.mean((y_true[:, 0, :]*torch.log(y_pred[:, 0, :]/(y_true[:, 0, :]+self.eps)+self.eps) + y_true[:, 1, :]*torch.log(y_pred[:, 1, :]/(y_true[:, 1, :]+self.eps)+self.eps) + y_true[:, 2, :]*torch.log(y_pred[:, 2, :]/(y_true[:, 2, :]+self.eps)+self.eps))*self.temp**2)\n",
    "        #x = -torch.sum(y_true[:, 0, :]*torch.log(y_pred[:, 0, :]/(y_true[:, 0, :]+self.eps)+self.eps) + y_true[:, 1, :]*torch.log(y_pred[:, 1, :]/(y_true[:, 1, :]+self.eps)+self.eps) + y_true[:, 2, :]*torch.log(y_pred[:, 2, :]/(y_true[:, 2, :]+self.eps)+self.eps))*self.temp**2\n",
    "        #return x/(y_pred.shape[0]*y_pred.shape[2])\n",
    "        \n",
    "#def weights_init(m):\n",
    "#    if isinstance(m, nn.Conv1d):\n",
    "#        if m.out_channels==3:\n",
    "#            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            #torch.nn.init.zeros_(m.bias)\n",
    "#            m.bias = nn.Parameter(torch.Tensor(np.log([n_null/(n_acceptor+n_donor),n_acceptor/(n_null+n_donor),n_donor/(n_acceptor+n_null)])))\n",
    "        #else:\n",
    "        #    torch.nn.init.kaiming_uniform_(m.weight)\n",
    "        #    if m.bias is not None:\n",
    "        #        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "        #        bound = 1 / np.sqrt(fan_in)\n",
    "        #        nn.init.uniform_(m.bias, -bound, bound)\n",
    "        \n",
    "#def weights_init(m):\n",
    "#    if isinstance(m, nn.Conv1d):\n",
    "#        if m.out_channels==3:\n",
    "#            torch.nn.init.xavier_uniform_(m.weight)\n",
    "#            #torch.nn.init.zeros_(m.bias)\n",
    "#            m.bias = nn.Parameter(torch.Tensor(np.log([n_null/(n_acceptor+n_donor),n_acceptor/(n_null+n_donor),n_donor/(n_acceptor+n_null)])))\n",
    "        #else:\n",
    "        #    torch.nn.init.kaiming_uniform_(m.weight)\n",
    "        #    if m.bias is not None:\n",
    "        #        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "        #        bound = 1 / np.sqrt(fan_in)\n",
    "        #        nn.init.uniform_(m.bias, -bound, bound)\n",
    "        \n",
    "        \n",
    "def keras_init(m):\n",
    "    if isinstance(m, nn.Conv1d): \n",
    "        fin, fout = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "        a = np.sqrt(6/(m.in_channels*(fin+fout)))\n",
    "        torch.nn.init.uniform_(m.weight, a=-a, b=a)\n",
    "        #print(m,nn.init._calculate_fan_in_and_fan_out(m.weight))\n",
    "        #nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        #nn.init.kaiming_uniform_(m.weight,mode='fan_in', nonlinearity='relu')\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        #if m.bias is not None:\n",
    "        #    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "        #    bound = 1 / np.sqrt(fan_in)\n",
    "        #    nn.init.uniform_(m.bias, -bound, bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10:  10%|██████                                                       | 20/202 [04:08<23:27,  7.73s/it, accepor_recall=0.112, donor_recall=0.194, loss=0.0077, loss_small=0.00849, pred_l1_dist=0.0193]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "hs = []\n",
    "#val_results_combined = []\n",
    "temp=1\n",
    "#for model_nr in range(5):\n",
    "for model_nr in range(5):\n",
    "    model_m = SpliceFormer(temp=1)\n",
    "    model_m.apply(keras_init)\n",
    "    model_m = model_m.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model_m = nn.DataParallel(model_m)\n",
    "    \n",
    "    modelFileName = '../Results/PyTorch_Models/transformer_encoder_40k_060422_{}'.format(model_nr)\n",
    "    #model_m.load_state_dict(torch.load('../Results/PyTorch_Models/SpliceAI_Ensembl_dgxtest_{}'.format(0)))\n",
    "    #loss = nn.CrossEntropyLoss(weight=torch.from_numpy(weights).float().to(device),ignore_index=-1,reduction='mean')\n",
    "    loss = categorical_crossentropy_2d().loss\n",
    "    #loss = nn.KLDivLoss()\n",
    "    #loss = \n",
    "    h = trainModel(model_m,modelFileName,loss,epochs,train_loader,val_loader,alpha=0.1,temp=temp)\n",
    "    hs.append(h)\n",
    "    #print(model_m.module.conv_final.bias)\n",
    "    #val_results_combined.append(val_results)\n",
    "    plt.plot(range(epochs),h['loss'],label='Train')\n",
    "    plt.plot(range(epochs),h['val_loss'],label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction-050422'\n",
    "setType = 'test'\n",
    "#with open('{}/sparse_sequence_data_{}.pickle'.format(data_dir,setType), 'rb') as handle:\n",
    "#    seqData = pickle.load(handle)\n",
    "    \n",
    "with open('{}/sparse_discrete_label_data_{}.pickle'.format(data_dir,setType), 'rb') as handle:\n",
    "    transcriptToLabel = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "CHROM_TEST = ['chr1', 'chr3', 'chr5', 'chr7', 'chr9']\n",
    "\n",
    "annotation = pd.read_csv(data_dir+'/annotation_ensembl_v87_{}.txt'.format(setType),sep='\\t',header=None)[[0,1,2,3,4]]\n",
    "annotation.columns = ['name','chrom','strand','tx_start','tx_end']\n",
    "annotation['transcript'] = annotation['name'].apply(lambda x: x.split('---')[-2].split('.')[0]).values\n",
    "annotation['gene'] = annotation['name'].apply(lambda x: x.split('---')[-3].split('.')[0]).values\n",
    "#annotation['support'] = annotation['transcript'].apply(lambda x:transcriptToSupport[x])\n",
    "\n",
    "chrom_paths = glob(data_dir+'/sparse_sequence_data/*')\n",
    "chromToPath = {}\n",
    "for path in chrom_paths:\n",
    "    chromToPath[path.split('/')[-1].split('_')[0]] = path\n",
    "    \n",
    "seqData = {}\n",
    "for chrom in CHROM_TEST:\n",
    "    seqData[chrom] = load_npz(data_dir+'/sparse_sequence_data/{}_{}.npz'.format(chrom,setType)).tocsr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = 1\n",
    "n_models = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(temp=temp)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_060422_{}'.format(i))) for i,model in enumerate(models)]\n",
    "#nr = [0,2,3]\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_201221_{}'.format(nr[i]))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(annotation)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=0,collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks.to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks.to(device),0),1,2)\n",
    "    batch_chunks = torch.split(batch_chunks, BATCH_SIZE, dim=0)\n",
    "    target_chunks = torch.split(target_chunks, BATCH_SIZE, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[1].detach() for i in range(n_models)])\n",
    "        outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "        targets_list.extend(targets.unsqueeze(0))\n",
    "        outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
