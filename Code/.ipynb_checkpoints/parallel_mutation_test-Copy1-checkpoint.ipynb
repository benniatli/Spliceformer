{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7feab078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from math import ceil\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "\n",
    "from src.train import trainModel\n",
    "from src.dataloader import get_GTEX_v8_Data,spliceDataset,h5pyDataset,getDataPointList,getDataPointListGTEX,DataPointGTEX\n",
    "from src.weight_init import keras_init\n",
    "from src.losses import categorical_crossentropy_2d\n",
    "from src.model import SpliceFormer\n",
    "from src.evaluation_metrics import print_topl_statistics\n",
    "import copy\n",
    "import pyfastx\n",
    "import gffutils\n",
    "\n",
    "global IN_MAP,BATCH_SIZE,CL_max,CL,SL,rev_comp_dict,device,n_models,models,base_map\n",
    "\n",
    "rev_comp_dict = {'A':'T','T':'A','C':'G','G':'C'}\n",
    "IN_MAP = np.asarray([[0, 0, 0, 0],\n",
    "                    [1, 0, 0, 0],\n",
    "                    [0, 1, 0, 0],\n",
    "                    [0, 0, 1, 0],\n",
    "                    [0, 0, 0, 1]])\n",
    "rng = np.random.default_rng(23673)\n",
    "#ii = int(sys.argv[1])\n",
    "L = 32\n",
    "N_GPUS = 4\n",
    "k = 3\n",
    "NUM_ACCUMULATION_STEPS=1\n",
    "# Hyper-parameters:\n",
    "# L: Number of convolution kernels\n",
    "# W: Convolution window size in each residual unit\n",
    "# AR: Atrous rate in each residual unit\n",
    "\n",
    "W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11,\n",
    "    21, 21, 21, 21, 41, 41, 41, 41])\n",
    "AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4,\n",
    "                 10, 10, 10, 10, 25, 25, 25, 25])\n",
    "BATCH_SIZE = 16*k*N_GPUS\n",
    "k = NUM_ACCUMULATION_STEPS*k\n",
    "CL = 2 * np.sum(AR*(W-1))\n",
    "\n",
    "SL=5000\n",
    "CL_max=40000\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7f1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_point_mutations(input_tensor, i, j,base_map,inside_gene,pos,base):\n",
    "    \"\"\"\n",
    "    Generate all point mutations within the range [i, j] for a batch of one-hot encoded genomic sequences.\n",
    "\n",
    "    Args:\n",
    "    input_tensor (torch.Tensor): Batch of one-hot encoded genomic sequences with shape (4, n).\n",
    "    i (int): Start index of the mutation range.\n",
    "    j (int): End index of the mutation range.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Batch of point mutations with shape (b, 4, n).\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract batch size and sequence length\n",
    "    #n_in_gene = np.sum(inside_gene[20000:25000])\n",
    "    n_in_gene = 5000\n",
    "    #_,_, n = input_tensor[:,:,n_in_gene].shape\n",
    "\n",
    "    # Create a copy of the input tensor to store point mutations\n",
    "    # Loop through the mutation range [i, j]\n",
    "    d = 0\n",
    "    #for batch in np.array_split(np.arange(i, i+n_in_gene),(n_in_gene)//(BATCH_SIZE//3)+1):\n",
    "    ref_base = []\n",
    "    alt_base = []\n",
    "    idx = []\n",
    "    c = 0\n",
    "    batch = np.arange(i, i+n_in_gene)\n",
    "    mutated_sequence = input_tensor.clone()\n",
    "    #for pos in batch:\n",
    "    original_base = torch.argmax(input_tensor[:, pos], dim=0).cpu().numpy()\n",
    "    possible_bases = [0,1,2,3]\n",
    "    possible_bases.remove(original_base)\n",
    "    #for base in possible_bases:\n",
    "    if inside_gene[pos]==1:\n",
    "        mutated_sequence[:, pos] = 0\n",
    "        mutated_sequence[base, pos] = 1\n",
    "        ref_base.append(base_map[original_base])\n",
    "        alt_base.append(base_map[base])\n",
    "        idx.append(d)\n",
    "    c += 1\n",
    "    d += 1\n",
    "    return mutated_sequence, alt_base, ref_base, np.array(idx)\n",
    "\n",
    "\n",
    "def getDeltas(delta,idx,inside_gene):\n",
    "    acceptorDelta = delta[:,1,:]*inside_gene\n",
    "    donorDelta = delta[:,2,:]*inside_gene\n",
    "    pos_gain_a = torch.argmax(acceptorDelta,dim=1)\n",
    "    pos_gain_d = torch.argmax(donorDelta,dim=1)\n",
    "    pos_loss_a = torch.argmax(-acceptorDelta,dim=1)\n",
    "    pos_loss_d = torch.argmax(-donorDelta,dim=1)\n",
    "\n",
    "    delta_gain_a = acceptorDelta.gather(1,pos_gain_a.unsqueeze(1)).cpu().numpy()[:,0]\n",
    "    delta_gain_d = donorDelta.gather(1,pos_gain_d.unsqueeze(1)).cpu().numpy()[:,0]\n",
    "    delta_loss_a = -acceptorDelta.gather(1,pos_loss_a.unsqueeze(1)).cpu().numpy()[:,0]\n",
    "    delta_loss_d = -donorDelta.gather(1,pos_loss_d.unsqueeze(1)).cpu().numpy()[:,0]\n",
    "    pos_gain_a = pos_gain_a.cpu().numpy()\n",
    "    pos_gain_d = pos_gain_d.cpu().numpy()\n",
    "    pos_loss_a = pos_loss_a.cpu().numpy()\n",
    "    pos_loss_d = pos_loss_d.cpu().numpy()\n",
    "    return delta_gain_a,delta_loss_a,delta_gain_d,delta_loss_d,pos_gain_a-idx,pos_loss_a-idx,pos_gain_d-idx,pos_loss_d-idx\n",
    "\n",
    "def ceil_div(x, y):\n",
    "    return int(ceil(float(x)/y))\n",
    "\n",
    "def one_hot_encode(Xd):\n",
    "    return IN_MAP[Xd.astype('int8')]\n",
    "\n",
    "def seqToArray(seq,strand):\n",
    "    seq = 'N'*(CL_max//2) + seq + 'N'*(CL_max//2)\n",
    "    seq = seq.upper()\n",
    "    seq = re.sub(r'[^AGTC]', '0',seq)\n",
    "    seq = seq.replace('A', '1').replace('C', '2')\n",
    "    seq = seq.replace('G', '3').replace('T', '4').replace('N', '0')\n",
    "    if strand == '+':\n",
    "        X0 = np.asarray([int(x) for x in seq])\n",
    "    elif strand == '-':\n",
    "        X0 = (5-np.asarray([int(x) for x in seq[::-1]])) % 5  # Reverse complement\n",
    "    return seq\n",
    "\n",
    "class mutation_dataset(Dataset):\n",
    "    def __init__(self, chrom, strand,start,end,ref,SL,CL_max,shift):\n",
    "        self.gene = gene\n",
    "        self.chrom = chrom\n",
    "        self.strand = strand\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.SL = SL\n",
    "        self.CL_max = CL_max\n",
    "        self.shift = shift\n",
    "        length = end-start+1\n",
    "        self.num_points = ceil_div(length, shift)\n",
    "        self.ref = 'N'*(CL_max//2) + ref + 'N'*(CL_max//2)\n",
    "        #self.ref = seqToArray(ref,strand)\n",
    "        self.length = (end-start)/SL\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_points*self.SL*3\n",
    "    \n",
    "    def reformat_data(X0):\n",
    "        # This function converts X0, Y0 of the create_datapoints function into\n",
    "        # blocks such that the data is broken down into data points where the\n",
    "        # input is a sequence of length SL+CL_max corresponding to SL nucleotides\n",
    "        # of interest and CL_max context nucleotides, the output is a sequence of\n",
    "        # length SL corresponding to the splicing information of the nucleotides\n",
    "        # of interest. The CL_max context nucleotides are such that they are\n",
    "        # CL_max/2 on either side of the SL nucleotides of interest.\n",
    "\n",
    "        #num_points = ceil_div(len(X0)-self.CL_max, self.SL)\n",
    "        Xd = np.zeros((self.num_points, self.SL+self.CL_max))\n",
    "        \n",
    "        #for i in range(num_points):\n",
    "        Xd[i] = X0[self.SL*i:self.CL_max+self.SL*(i+1)]\n",
    "\n",
    "        return Xd\n",
    "    \n",
    "    def getData(self,seq,idx):\n",
    "        X = np.zeros((5,self.SL+self.CL_max))\n",
    "        seq = seq[int((CL_max+SL)*idx):int((CL_max+SL)*(idx+1))]\n",
    "        if len(seq)<self.SL+self.CL_max:\n",
    "            if strand=='+':\n",
    "                seq = seq+'N'*(self.SL+self.CL_max-len(seq))\n",
    "            else:\n",
    "                seq = 'N'*(self.SL+self.CL_max-len(seq))+seq\n",
    "        seq = seq.upper()\n",
    "        seq = re.sub(r'[^AGTC]', '0',seq)\n",
    "\n",
    "        seq = seq.replace('A', '1').replace('C', '2')\n",
    "        seq = seq.replace('G', '3').replace('T', '4').replace('N', '0')\n",
    "\n",
    "        if strand == '+':\n",
    "            X0 = np.asarray([int(x) for x in seq])\n",
    "            \n",
    "        elif strand == '-':\n",
    "            X0 = (5-np.asarray([int(x) for x in seq[::-1]])) % 5  \n",
    "\n",
    "        \n",
    "        X[X0,np.arange(CL_max+SL)] = 1 \n",
    "        \n",
    "        return X[1:,:].copy()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx,pos = idx//(self.SL*3),idx%(self.SL*3)\n",
    "        pos,base = pos//3,pos%3\n",
    "        X = self.getData(self.ref,idx)\n",
    "        #batch_features = torch.Tensor(ref).type(torch.FloatTensor).unsqueeze(dim=0).to(device)\n",
    "        #ref_pred = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "        #ref_pred = torch.stack(ref_pred)\n",
    "        #ref_pred = torch.mean(ref_pred,dim=0)\n",
    "        inside_gene = (X.sum(axis=(0)) >= 1)\n",
    "        mutation,alt_base,ref_base,idx = generate_point_mutations(torch.Tensor(X), self.CL_max//2, self.CL_max//2+SL,base_map,inside_gene,pos,base)\n",
    "        return X,mutation\n",
    "\n",
    "def predictMutations(ref,strand,chrom,shift,start,end):\n",
    "    for i,(mutation,alt_base,ref_base,idx) in enumerate(mutations):\n",
    "        \n",
    "        mutation = mutation.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        alt_pred = ([models[i](mutation)[0].detach() for i in range(n_models)])\n",
    "        alt_pred = torch.stack(alt_pred)\n",
    "        alt_pred = torch.mean(alt_pred,dim=0)\n",
    "\n",
    "        delta = alt_pred-ref_pred\n",
    "        a1,b1,c1,d1,a2,b2,c2,d2 = getDeltas(delta,CL_max//2+idx,inside_gene)\n",
    "\n",
    "        if strand == '+':\n",
    "            df = pd.DataFrame({'CHROM':chrom,'POS':start+shift+idx,'REF':ref_base,'ALT':alt_base,'DS_AG':a1,'DS_AL':b1,'DS_DG':c1,'DS_DL':d1,'DP_AG':a2,'DP_AL':b2,'DP_DG':c2,'DP_DL':d2})\n",
    "        else:\n",
    "            df = pd.DataFrame({'CHROM':chrom,'POS':end-shift-idx,'REF':[rev_comp_dict[x] for x in ref_base],'ALT':[rev_comp_dict[x] for x in alt_base],'DS_AG':a1,'DS_AL':b1,'DS_DG':c1,'DS_DL':d1,'DP_AG':-a2,'DP_AL':-b2,'DP_DG':-c2,'DP_DL':-d2})\n",
    "        if i==0:\n",
    "            results = df\n",
    "        else:\n",
    "            results = pd.concat([results,df],axis=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9b27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf = gffutils.FeatureDB(\"/odinn/tmp/benediktj/Data/Gencode_V44/gencode.v44.annotation.db\")\n",
    "fasta = pyfastx.Fasta('/odinn/tmp/benediktj/Data/Gencode_V44/GRCh38.p14.genome.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84243fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62700it [00:19, 3283.04it/s]\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "for gene in tqdm(gtf.features_of_type('gene')):\n",
    "    if gene['gene_type'][0] == \"protein_coding\" and gene[0] != 'chrM':\n",
    "        df.append([gene[0],gene[3],gene[4],gene[6],gene['gene_name'][0],gene['gene_id'][0]])\n",
    "    #print(gene)\n",
    "      #  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fcefb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/20033 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = ['CHROM','START','END','STRAND','NAME','ID']\n",
    "\n",
    "#df = np.array_split(df,300)[ii]\n",
    "\n",
    "base_map = 'ACGT'\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    gene_name,chrom,strand,start,end,gene_id = df.iloc[i,:][['NAME','CHROM','STRAND','START','END','ID']]\n",
    "    ref = fasta[chrom][start-1:end].seq\n",
    "    mutations = mutation_dataset(chrom, strand,start,end,ref,SL,CL_max,SL)\n",
    "    break\n",
    "    #ref = seqToArray(ref,strand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ffccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(mutations, batch_size=BATCH_SIZE, shuffle=False, num_workers=32, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660afc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6167"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1009e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSplicing(seq,models):\n",
    "    outputs = []\n",
    "    for i in range(seq.shape[0]):\n",
    "        batch_features = torch.tensor(seq[i,:,:], device=device).float().unsqueeze(0)\n",
    "        batch_features = torch.swapaxes(batch_features,1,2)\n",
    "       \n",
    "    \n",
    "    outputs = torch.cat(outputs,dim=2)\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a0ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACCUMULATION_STEPS = 1\n",
    "temp = 1\n",
    "n_models = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = SpliceFormer(CL_max,bn_momentum=0.01/NUM_ACCUMULATION_STEPS,depth=4,heads=4,n_transformer_blocks=2,determenistic=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_finetune_rnasplice-blood_all_050623_{}'.format(i))) for i,model in enumerate(models)]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c2990d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████▌                                                                | 38/157 [06:00<18:49,  9.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_703599/647399627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mref_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mref_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0malt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0malt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malt_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_703599/647399627.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mref_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mref_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0malt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0malt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malt_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for (ref,alt) in tqdm(loader):\n",
    "    ref = ref.to(device).float()\n",
    "    alt = alt.to(device).float()\n",
    "    ref_pred = ([models[i](ref)[0].detach() for i in range(n_models)])\n",
    "    ref_pred = torch.mean(torch.stack(ref_pred),dim=0)\n",
    "    alt_pred = ([models[i](alt)[0].detach() for i in range(n_models)])\n",
    "    alt_pred = torch.mean(torch.stack(alt_pred),dim=0)\n",
    "    delta = ref_pred-alt_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012166a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
