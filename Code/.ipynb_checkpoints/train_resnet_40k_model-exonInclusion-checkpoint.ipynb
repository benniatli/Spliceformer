{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from math import ceil\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#import pickle5 as pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "from glob import glob\n",
    "\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "import copy\n",
    "\n",
    "from src.train import trainModel\n",
    "from src.dataloader import getData,spliceDataset,h5pyDataset,collate_fn\n",
    "from src.weight_init import keras_init\n",
    "from src.losses import categorical_crossentropy_2d\n",
    "from src.models import ResNet_40K\n",
    "from src.evaluation_metrics import print_topl_statistics\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 12:05:19 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    58W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  A100-SXM4-40GB      On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    55W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  A100-SXM4-40GB      On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    53W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  A100-SXM4-40GB      On   | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    55W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  A100-SXM4-40GB      On   | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    54W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  A100-SXM4-40GB      On   | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    58W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  A100-SXM4-40GB      On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    55W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  A100-SXM4-40GB      On   | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    56W / 400W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(23673)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Model\n",
    "###############################################################################\n",
    "\n",
    "L = 32\n",
    "N_GPUS = 8\n",
    "k = 2\n",
    "\n",
    "# Hyper-parameters:\n",
    "# L: Number of convolution kernels\n",
    "# W: Convolution window size in each residual unit\n",
    "# AR: Atrous rate in each residual unit\n",
    "W = np.asarray([11, 11, 11, 11, 11, 11, 11, 11,\n",
    "                21, 21, 21, 21, 41, 41, 41, 41, 51, 51, 51, 51])\n",
    "AR = np.asarray([1, 1, 1, 1, 4, 4, 4, 4,\n",
    "                 10, 10, 10, 10, 25, 25, 25, 25, 75, 75, 75, 75])\n",
    "BATCH_SIZE = 6*k*N_GPUS\n",
    "\n",
    "CL = 2 * np.sum(AR*(W-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction-050422'\n",
    "setType = 'train'\n",
    "\n",
    "annotation, transcriptToLabel, seqData = getData(data_dir, setType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_max=40000\n",
    "# Maximum nucleotide context length (CL_max/2 on either side of the \n",
    "# position of interest)\n",
    "# CL_max should be an even number\n",
    "SL=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert CL_max % 2 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gene, validation_gene = train_test_split(annotation['gene'].drop_duplicates(),test_size=.1,random_state=435)\n",
    "annotation_train = annotation[annotation['gene'].isin(train_gene)]\n",
    "annotation_validation = annotation[annotation['gene'].isin(validation_gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = spliceDataset(annotation_train,transcriptToLabel,SL,CL_max,exonInclusion=True)\n",
    "val_dataset = spliceDataset(annotation_validation,transcriptToLabel,SL,CL_max,exonInclusion=True)\n",
    "train_dataset.seqData = seqData\n",
    "val_dataset.seqData = seqData\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=k*100, shuffle=True, num_workers=16,collate_fn=collate_fn, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=k*100, shuffle=False,collate_fn=collate_fn, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10: 100%|██████████████████████████████████████████████████████| 101/101 [15:43<00:00,  9.34s/it, accepor_recall=0.837, donor_recall=0.856, loss=0.000523, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10, train loss = 0.009850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 1/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:44<00:00,  8.67s/it, accepor_recall=0.912, donor_recall=0.932, loss=0.000964, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9624\t0.8591\t0.9689\t0.9846\t0.91\t0.9776999950408936\t0.7470999956130981\t0.0794999971985817\t0.009499999694526196\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9618\t0.8632\t0.9748\t0.9893\t0.9154\t0.9857000112533569\t0.832099974155426\t0.0851999968290329\t0.007799999788403511\t21432\n",
      "epoch: 1/10, val loss = 0.000874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 2/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:49<00:00,  8.81s/it, accepor_recall=0.862, donor_recall=0.877, loss=0.000465, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/10, train loss = 0.000465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 2/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:51<00:00,  9.25s/it, accepor_recall=0.876, donor_recall=0.883, loss=0.000767, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9665\t0.8765\t0.9731\t0.9876\t0.9208\t0.9718000292778015\t0.48339998722076416\t0.011500000022351742\t0.001500000013038516\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9686\t0.8862\t0.9788\t0.9912\t0.9305\t0.9606000185012817\t0.47130000591278076\t0.00989999994635582\t0.0013000000035390258\t21432\n",
      "epoch: 2/10, val loss = 0.000672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 3/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:58<00:00,  8.90s/it, accepor_recall=0.895, donor_recall=0.903, loss=0.000366, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/10, train loss = 0.000415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 3/10: 100%|███████████████████████████████████████████████████████████| 12/12 [01:50<00:00,  9.19s/it, accepor_recall=0.894, donor_recall=0.948, loss=0.00135, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.923\t0.8075\t0.9479\t0.9794\t0.8561\t0.907800018787384\t0.7253000140190125\t0.16699999570846558\t0.01860000006854534\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9542\t0.8419\t0.9641\t0.9861\t0.8955\t0.9771999716758728\t0.895799994468689\t0.31200000643730164\t0.039400000125169754\t21432\n",
      "epoch: 3/10, val loss = 0.001241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 4/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:53<00:00,  8.84s/it, accepor_recall=0.902, donor_recall=0.909, loss=0.000368, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/10, train loss = 0.000379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 4/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:51<00:00,  9.28s/it, accepor_recall=0.912, donor_recall=0.912, loss=0.000689, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9637\t0.8784\t0.9759\t0.9876\t0.92\t0.9440000057220459\t0.6488999724388123\t0.04089999943971634\t0.0052999998442828655\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.965\t0.8806\t0.9776\t0.9904\t0.9254\t0.9463000297546387\t0.6287000179290771\t0.02500000037252903\t0.002400000113993883\t21432\n",
      "epoch: 4/10, val loss = 0.000614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 5/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:49<00:00,  8.81s/it, accepor_recall=0.908, donor_recall=0.913, loss=0.000329, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/10, train loss = 0.000341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 5/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:47<00:00,  8.94s/it, accepor_recall=0.916, donor_recall=0.885, loss=0.000598, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9706\t0.8934\t0.9809\t0.9907\t0.9349\t0.9684000015258789\t0.6410999894142151\t0.018799999728798866\t0.0020000000949949026\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9684\t0.9008\t0.9848\t0.9932\t0.9403\t0.8898000121116638\t0.4284000098705292\t0.009800000116229057\t0.0008999999845400453\t21432\n",
      "epoch: 5/10, val loss = 0.000515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 6/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:54<00:00,  8.86s/it, accepor_recall=0.918, donor_recall=0.923, loss=0.000284, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6/10, train loss = 0.000315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 6/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:50<00:00,  9.17s/it, accepor_recall=0.895, donor_recall=0.917, loss=0.000518, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9715\t0.9128\t0.9838\t0.9914\t0.9448\t0.9585999846458435\t0.4562000036239624\t0.008500000461935997\t0.0017000000225380063\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9707\t0.9118\t0.9884\t0.9944\t0.9481\t0.9726999998092651\t0.5709999799728394\t0.01080000028014183\t0.0015999999595806003\t21432\n",
      "epoch: 6/10, val loss = 0.000435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 7/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:40<00:00,  8.72s/it, accepor_recall=0.926, donor_recall=0.931, loss=0.000301, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7/10, train loss = 0.000291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 7/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:43<00:00,  8.59s/it, accepor_recall=0.863, donor_recall=0.884, loss=0.000517, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9706\t0.9151\t0.9824\t0.9888\t0.9448\t0.9337000250816345\t0.27090001106262207\t0.002400000113993883\t0.0005000000237487257\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9721\t0.9208\t0.9874\t0.9932\t0.9512\t0.9550999999046326\t0.30489999055862427\t0.002099999925121665\t0.00039999998989515007\t21432\n",
      "epoch: 7/10, val loss = 0.000426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 8/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:46<00:00,  8.78s/it, accepor_recall=0.944, donor_recall=0.947, loss=0.000218, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8/10, train loss = 0.000232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 8/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:41<00:00,  8.45s/it, accepor_recall=0.896, donor_recall=0.879, loss=0.000561, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9733\t0.9203\t0.9855\t0.9911\t0.9499\t0.949999988079071\t0.3970000147819519\t0.004699999932199717\t0.0008999999845400453\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9756\t0.9232\t0.989\t0.9949\t0.9551\t0.9169999957084656\t0.2766000032424927\t0.002199999988079071\t0.00039999998989515007\t21432\n",
      "epoch: 8/10, val loss = 0.000419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 9/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:49<00:00,  8.81s/it, accepor_recall=0.953, donor_recall=0.954, loss=0.000189, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/10, train loss = 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 9/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:45<00:00,  8.77s/it, accepor_recall=0.921, donor_recall=0.927, loss=0.000449, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9737\t0.9284\t0.9875\t0.9922\t0.9534\t0.9740999937057495\t0.5267000198364258\t0.004800000227987766\t0.0007999999797903001\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9745\t0.9295\t0.9902\t0.9948\t0.9562\t0.979200005531311\t0.5805000066757202\t0.003800000064074993\t0.0006000000284984708\t21432\n",
      "epoch: 9/10, val loss = 0.000373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 10/10: 100%|█████████████████████████████████████████████████████| 101/101 [14:46<00:00,  8.77s/it, accepor_recall=0.958, donor_recall=0.959, loss=0.000195, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10/10, train loss = 0.000180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 10/10: 100%|█████████████████████████████████████████████████████████| 12/12 [01:53<00:00,  9.49s/it, accepor_recall=0.913, donor_recall=0.918, loss=0.000476, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9741\t0.9265\t0.9871\t0.9925\t0.9546\t0.9854000210762024\t0.45680001378059387\t0.0027000000700354576\t0.0005000000237487257\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9754\t0.9295\t0.9899\t0.9943\t0.9571\t0.9883999824523926\t0.48969998955726624\t0.002099999925121665\t0.0003000000142492354\t21432\n",
      "epoch: 10/10, val loss = 0.000378\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10: 100%|██████████████████████████████████████████████████████| 101/101 [14:55<00:00,  8.87s/it, accepor_recall=0.844, donor_recall=0.858, loss=0.000497, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10, train loss = 0.008602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 1/10: 100%|██████████████████████████████████████████████████████████| 12/12 [01:52<00:00,  9.35s/it, accepor_recall=0.774, donor_recall=0.885, loss=0.000999, pred_l1_dist=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.9479\t0.81\t0.9355\t0.9732\t0.8671\t0.8718000054359436\t0.3752000033855438\t0.029999999329447746\t0.00430000014603138\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.9598\t0.838\t0.9576\t0.9846\t0.8961\t0.9799000024795532\t0.7347999811172485\t0.0681999996304512\t0.008500000461935997\t21432\n",
      "epoch: 1/10, val loss = 0.000901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 2/10:   7%|███▉                                                    | 7/101 [01:26<13:49,  8.82s/it, accepor_recall=0.832, donor_recall=0.847, loss=0.000527, pred_l1_dist=0]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "hs = []\n",
    "for model_nr in range(5):\n",
    "    model_m = ResNet_40K(CL_max,exonInclusion=True)\n",
    "    model_m.apply(keras_init)\n",
    "    model_m = model_m.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model_m = nn.DataParallel(model_m)\n",
    "    \n",
    "    modelFileName = '../Results/PyTorch_Models/resnet_encoder_40k_170522_{}'.format(model_nr)\n",
    "    #model_m.load_state_dict(torch.load('../Results/PyTorch_Models/SpliceAI_Ensembl_dgxtest_{}'.format(0)))\n",
    "    #loss = nn.CrossEntropyLoss(weight=torch.from_numpy(weights).float().to(device),ignore_index=-1,reduction='mean')\n",
    "    loss = categorical_crossentropy_2d().loss\n",
    "    learning_rate= 1e-3*k\n",
    "    optimizer = torch.optim.Adam(model_m.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "    warmup = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\n",
    "    h = trainModel(model_m,modelFileName,loss,train_loader,val_loader,optimizer,scheduler,warmup,BATCH_SIZE,epochs,device,exonInclusion=True)\n",
    "    hs.append(h)\n",
    "    #print(model_m.module.conv_final.bias)\n",
    "    #val_results_combined.append(val_results)\n",
    "    plt.plot(range(epochs),h['loss'],label='Train')\n",
    "    plt.plot(range(epochs),h['val_loss'],label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "h5f = h5py.File('/odinn/tmp/benediktj/Data/SplicePrediction/gencode_40k_dataset_test_.h5')\n",
    "\n",
    "num_idx = len(h5f.keys())//2\n",
    "\n",
    "test_dataset = h5pyDataset(h5f,list(range(num_idx)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "temp = 1\n",
    "n_models = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = ResNet_40K(CL_max,exonInclusion=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/resnet_encoder_40k_170522_{}'.format(i))) for i,model in enumerate(models)]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks[0].to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks[0].to(device),0),1,2)\n",
    "    #print(np.max(target_chunks.cpu().numpy()[:,2,:]))\n",
    "    n_chunks = int(np.ceil(batch_chunks.shape[0]/BATCH_SIZE))\n",
    "    batch_chunks = torch.chunk(batch_chunks, n_chunks, dim=0)\n",
    "    target_chunks = torch.chunk(target_chunks, n_chunks, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "        outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "        targets_list.extend(targets.unsqueeze(0))\n",
    "        outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/odinn/tmp/benediktj/Data/SplicePrediction-050422'\n",
    "setType = 'test'\n",
    "annotation_test, transcriptToLabel_test, seqData = getData(data_dir, setType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = 1\n",
    "n_models = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = ResNet_40K(CL_max,exonInclusion=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/resnet_encoder_40k_170522_{}'.format(i))) for i,model in enumerate(models)]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "#nr = [0,2,3]\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_201221_{}'.format(nr[i]))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(annotation_test,transcriptToLabel_test,SL,CL_max)\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=0,collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks.to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks.to(device),0),1,2)\n",
    "    batch_chunks = torch.split(batch_chunks, BATCH_SIZE, dim=0)\n",
    "    target_chunks = torch.split(target_chunks, BATCH_SIZE, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[0].detach() for i in range(n_models)])\n",
    "        outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "        targets_list.extend(targets.unsqueeze(0))\n",
    "        outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "\n",
    "    is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1\n",
    "n_models = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_m = ResNet_40K(CL_max,exonInclusion=True)\n",
    "model_m.apply(keras_init)\n",
    "model_m = model_m.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_m = nn.DataParallel(model_m)\n",
    "\n",
    "output_class_labels = ['Null', 'Acceptor', 'Donor']\n",
    "\n",
    "#for output_class in [1,2]:\n",
    "models = [copy.deepcopy(model_m) for i in range(n_models)]\n",
    "[model.load_state_dict(torch.load('../Results/PyTorch_Models/resnet_encoder_40k_170522_{}'.format(i))) for i,model in enumerate(models)]\n",
    "\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    \n",
    "#nr = [0,2,3]\n",
    "#[model.load_state_dict(torch.load('../Results/PyTorch_Models/transformer_encoder_40k_201221_{}'.format(nr[i]))) for i,model in enumerate(models)]\n",
    "#chunkSize = num_idx/10\n",
    "\n",
    "\n",
    "Y_true_acceptor, Y_pred_acceptor = [],[]\n",
    "Y_true_donor, Y_pred_donor = [],[]\n",
    "test_dataset = spliceDataset(annotation_test,transcriptToLabel_test,SL,CL_max,exonInclusion=True)\n",
    "test_dataset.seqData = seqData\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=0,collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "\n",
    "for (batch_chunks,target_chunks) in tqdm(test_loader):\n",
    "    batch_chunks = torch.transpose(batch_chunks.to(device),1,2)\n",
    "    target_chunks = torch.transpose(torch.squeeze(target_chunks.to(device),0),1,2)\n",
    "    batch_chunks = torch.split(batch_chunks, BATCH_SIZE, dim=0)\n",
    "    target_chunks = torch.split(target_chunks, BATCH_SIZE, dim=0)\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    for j in range(len(batch_chunks)):\n",
    "        batch_features = batch_chunks[j]\n",
    "        targets = target_chunks[j]\n",
    "        outputs = ([models[i](batch_features)[1].detach() for i in range(n_models)])\n",
    "        outputs = (outputs[0]+outputs[1]+outputs[2]+outputs[3]+outputs[4])/n_models\n",
    "        #print(outputs.cpu().numpy().shape)\n",
    "        plt.plot(outputs.cpu().numpy()[0,0,:],label='Predicted')\n",
    "        #print(targets.cpu().numpy().shape)\n",
    "        tmp = targets.cpu().numpy()\n",
    "        plt.plot(tmp[0,3,:],label='Exon')\n",
    "        \n",
    "        acceptor = tmp[0,1,:]==1\n",
    "        donor = tmp[0,2,:]==1\n",
    "        plt.ylim([0,1.1])\n",
    "        plt.scatter(np.arange(5000)[acceptor],tmp[0,3,acceptor],label='Acceptor')\n",
    "        plt.scatter(np.arange(5000)[donor],tmp[0,3,donor],label='Donor')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        #outputs = (outputs[0]+outputs[1]+outputs[2])/n_models\n",
    "   #     targets_list.extend(targets.unsqueeze(0))\n",
    "   #     outputs_list.extend(outputs.unsqueeze(0))\n",
    "\n",
    "    #targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "    #outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "\n",
    "    #is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "    #Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "    #Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "    #Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "    #Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
