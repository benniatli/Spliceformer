{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88bda3a-19da-40c4-a1af-4399391fc1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/wdr')\n",
    "from standalone_hyenadna import HyenaDNAModel, CharacterTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bcedec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 30 17:40:06 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.8     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:31:00.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    40W / 250W |      0MiB / 40960MiB |      0%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA A100-PCI...  Off  | 00000000:98:00.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    42W / 250W |      0MiB / 40960MiB |      0%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  NVIDIA A100-PCI...  Off  | 00000000:CA:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P0    39W / 250W |      0MiB / 40960MiB |     29%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef763d34-1d6c-4a39-b6e3-cbaccf28d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import transformers\n",
    "from transformers import PreTrainedModel #, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "\n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        config=None,\n",
    "                        device='cuda',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if config is None:\n",
    "            config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15222b2e-4e72-41f2-b1d7-ebece469baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from standalone_hyenadna import LMBackbone, _init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715a82c6-bc3e-4bb2-a023-beb08dcfe0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyenaDNAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if vocab_size % pad_vocab_size_multiple != 0:\n",
    "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
    "\n",
    "        self.use_head = use_head\n",
    "\n",
    "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
    "        if 'd_model' not in layer:\n",
    "            layer['d_model'] = d_model\n",
    "\n",
    "        self.backbone = LMBackbone(\n",
    "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
    "            **factory_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "        # linear head for token classification\n",
    "        if self.use_head:\n",
    "            self.head = torch.nn.Linear(d_model, n_classes)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
    "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
    "\n",
    "        if self.use_head:\n",
    "            return self.head(hidden_states)\n",
    "        else:\n",
    "            return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb74c761-baeb-41f9-8537-cbf578b7851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "# pretrained_model_name = 'hyenadna-tiny-1k-seqlen'\n",
    "pretrained_model_name = 'hyenadna-small-32k-seqlen'\n",
    "# pretrained_model_name = 'hyenadna-medium-160k-seqlen'\n",
    "\n",
    "\n",
    "max_lengths = {\n",
    "    'hyenadna-tiny-1k-seqlen': 1024,\n",
    "    'hyenadna-small-32k-seqlen': 32768,\n",
    "    'hyenadna-medium-160k-seqlen': 160000,\n",
    "    'hyenadna-medium-450k-seqlen': 450000,  # T4 up to here\n",
    "    'hyenadna-large-1m-seqlen': 1_000_000,  # only A100 (paid tier)\n",
    "}\n",
    "max_length = max_lengths[pretrained_model_name]\n",
    "\n",
    "with open(os.path.join('/odinn/tmp/benediktj/', pretrained_model_name, 'config.json')) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# data settings:\n",
    "use_padding = True\n",
    "rc_aug = False  # reverse complement augmentation\n",
    "add_eos = False  # add end of sentence token\n",
    "\n",
    "# we need these for the decoder head, if using\n",
    "use_head = True\n",
    "n_classes = 3  # not used for embeddings only\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    '/odinn/tmp/benediktj/',\n",
    "    pretrained_model_name,\n",
    "    config=None,\n",
    "    device=device,\n",
    "    use_head=use_head,\n",
    "    n_classes=n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26a3fe3-530c-49dc-bf3e-4650905badec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create tokenizer\n",
    "# tokenizer = CharacterTokenizer(\n",
    "#     characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "#     model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "#     add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "#     padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "# )\n",
    "\n",
    "#### Single embedding example ####\n",
    "\n",
    "# create a sample 450k long, prepare\n",
    "# sequence = 'ACTG' * int(max_length/4)\n",
    "# tok_seq = tokenizer(sequence)\n",
    "# tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
    "\n",
    "# # place on device, convert to tensor\n",
    "# tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "# tok_seq = tok_seq.to(device)\n",
    "\n",
    "# # prep model and forward\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "# with torch.inference_mode():\n",
    "#     embeddings = model(tok_seq)\n",
    "# embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c190baf-6941-4b2b-8b99-1721528dec2f",
   "metadata": {},
   "source": [
    "# splicing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957c1f43-f54e-45be-9c4b-d1d89f74297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from src.dataloader import getData\n",
    "\n",
    "\n",
    "class spliceDataset(Dataset):\n",
    "    def __init__(self, annotation, transform=None, target_transform=None):\n",
    "        self.annotation = annotation\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.vocab = { # from hyenaDNA tokenizer\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            'A':7,\n",
    "            'C':8,\n",
    "            'G':9,\n",
    "            'T':10,\n",
    "            'N':11\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X,Y = self.annotation[idx].getData(self.seqData)\n",
    "        \n",
    "        # convert from one-hot to index, ['A', 'C', 'G', 'T', 'N']\n",
    "        # make it 'N' if all elements in X are 0\n",
    "        Ncharacter = X.sum(axis=0) == 0\n",
    "        sequence = X.argmax(axis=0)\n",
    "        sequence[Ncharacter] = 4\n",
    "        \n",
    "        # to match hyenaDNA tokenizer\n",
    "        sequence += 7\n",
    "        \n",
    "        # add CLS token to start and SEP token to end of sequence\n",
    "        sequence = np.concatenate([[self.vocab['[CLS]']], sequence, [self.vocab['[SEP]']]])\n",
    "        \n",
    "        #  convert Y also from one hot to idx, assumes it is a valid one-hot vector (always 1 in one and only one of the positions)\n",
    "        #Y = Y.argmax(axis=0)\n",
    "        \n",
    "        return sequence, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee6c243-51f1-48d5-aec1-7ca348cfe71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataloader import getData,getDataPointListFull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f60f5f-b77d-47bf-9165-a97ef3ccb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Data'\n",
    "setType = 'train'\n",
    "# label \n",
    "annotation, transcriptToLabel, seqData = getData(data_dir, setType)\n",
    "\n",
    "SL=5000 # |---------5000-------------|\n",
    "CL_max=max_length-SL-2 #  ----30000----|-----5000-----|------30000------- # base: 40000 \n",
    "\n",
    "train_gene, validation_gene = train_test_split(annotation['gene'].drop_duplicates(),test_size=.1,random_state=435)\n",
    "annotation_train = annotation[annotation['gene'].isin(train_gene)]\n",
    "annotation_validation = annotation[annotation['gene'].isin(validation_gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53df49b7-a0e2-49a0-b2df-5223a09e9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = spliceDataset(getDataPointListFull(annotation_train,transcriptToLabel,SL,CL_max,shift=SL))\n",
    "val_dataset = spliceDataset(getDataPointListFull(annotation_validation,transcriptToLabel,SL,CL_max,shift=SL))\n",
    "train_dataset.seqData = seqData\n",
    "val_dataset.seqData = seqData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482282d0-5b95-4e0c-ab25-5cd031909818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20116"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978ca4dd-96e1-4584-98d8-dfe082a1210d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26790"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1337825b-e19a-4b72-926e-ceb9b116766d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8955"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label \n",
    "annotation_test, transcriptToLabel, seqData = getData(data_dir, \"test\")\n",
    "len(annotation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "224515b2-bdf8-4cee-a879-c6b8e0e00082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77332dc5-ea94-409e-8e22-953cbc60acdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  7, 10, ..., 11, 11,  1]),\n",
       " array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[739]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "541e890c-0682-447c-ae05-cba7c57025aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx: 739\n",
    "#mask_l: 0 mask_r: 699\n",
    "#a: 0 -201 b: -201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b14785d8-f68e-4f00-bff7-93f399f8e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9792e0c6-50d5-4af1-958c-6992d73bcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\"\"\"\n",
    "We provide simple training code for the GenomicBenchmark datasets.\n",
    "\"\"\"\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, loss_fn, log_interval=10):\n",
    "    \"\"\"Training loop.\"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if batch_idx % log_interval == 0:\n",
    "        #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    \"\"\"Test loop.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target.squeeze()).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f13651b5-063d-4a9e-b61d-215428494e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a9d4665-0833-4142-b8d8-4f09a6a82918",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef0fc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import trainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d13428db-b79e-443a-9071-e62ab852f790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): HyenaDNAModel(\n",
       "    (backbone): LMBackbone(\n",
       "      (embeddings): GPT2Embeddings(\n",
       "        (word_embeddings): Embedding(16, 256)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_f): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (head): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "'''\n",
    "Main entry point for training.  Select the dataset name and metadata, as\n",
    "well as model and training args, and you're off to the genomic races!\n",
    "\n",
    "### GenomicBenchmarks Metadata\n",
    "# there are 8 datasets in this suite, choose 1 at a time, with their corresponding settings\n",
    "# name                                num_seqs        num_classes     median len    std\n",
    "# dummy_mouse_enhancers_ensembl       1210            2               2381          984.4\n",
    "# demo_coding_vs_intergenomic_seqs    100_000         2               200           0\n",
    "# demo_human_or_worm                  100_000         2               200           0\n",
    "# human_enhancers_cohn                27791           2               500           0\n",
    "# human_enhancers_ensembl             154842          2               269           122.6\n",
    "# human_ensembl_regulatory            289061          3               401           184.3\n",
    "# human_nontata_promoters             36131           2               251           0\n",
    "# human_ocr_ensembl                   174756          2               315           108.1\n",
    "\n",
    "'''\n",
    "# experiment settings:\n",
    "num_epochs = 100  # ~100 seems fine\n",
    "#learning_rate = 6e-4  # good default for Hyena\n",
    "learning_rate= 1e-3\n",
    "#weight_decay = 0.1\n",
    "ngpu = 3\n",
    "batch_size = ngpu*2*16\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=True)\n",
    "\n",
    "\n",
    "# we need these for the decoder head, if using\n",
    "use_head = True\n",
    "n_classes = 3\n",
    "\n",
    "#model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "#    '/odinn/tmp/benediktj/',\n",
    "#    pretrained_model_name,\n",
    "#    config=None,\n",
    "#    device=device,\n",
    "#    use_head=use_head,\n",
    "#    n_classes=n_classes,\n",
    "#)\n",
    "\n",
    "# loss function\n",
    "def loss_fn(output, target):\n",
    "    # output shape: bs, seql+2, n_classes\n",
    "    # target shape: bs, seql\n",
    "    \n",
    "    # only the window ----| SL |----\n",
    "    output = output[:, CL_max//2 + 1:CL_max//2 + 1 + SL]\n",
    "    target = target[:, CL_max//2:CL_max//2 + SL]\n",
    "    print(output.shape,target.shape)\n",
    "    # cross entropy expects (N, C, d1, d2, ...)\n",
    "    output = torch.moveaxis(output, -1, 1)\n",
    "    \n",
    "    # output shape: bs, n_classes, SL\n",
    "    # target shape: bs, SL\n",
    "    ce = torch.nn.functional.cross_entropy(output, target)\n",
    "    loss = torch.mean(ce)\n",
    "    return loss\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "        #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "#    train(model, device, train_loader, optimizer, epoch, loss_fn)\n",
    "#    test(model, device, val_loader, loss_fn)\n",
    "#   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9308f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_constant_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a49dcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27766"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CL_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc57a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from src.evaluation_metrics import print_topl_statistics\n",
    "from src.losses import binary_crossentropy_2d\n",
    "import pandas as pd\n",
    "\n",
    "def trainModel(model,fileName,criterion,train_loader,val_loader,optimizer,scheduler,warmup,BATCH_SIZE,epochs,device,verbose=1,CL_max=40000,lowValidationGPUMem=False,skipValidation=False,NUM_ACCUMULATION_STEPS=1,reinforce=True,continous_labels=False,no_softmax=False):\n",
    "    losses = {}\n",
    "    losses['train'] = []\n",
    "    losses['val'] = []\n",
    "    val_results = []\n",
    "    dataLoaders = {}\n",
    "    dataLoaders['train'] = train_loader\n",
    "    dataLoaders['val'] = val_loader\n",
    "    multiplier = 0.01\n",
    "    eps = torch.finfo(torch.float32).eps\n",
    "    acceptor_acc_avg = 0\n",
    "    donor_acc_avg = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['train','val']:\n",
    "            if skipValidation and (phase=='val'):\n",
    "                continue\n",
    "            loop =tqdm(dataLoaders[phase])\n",
    "            if phase=='train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            loss = 0\n",
    "            ema_loss = 0\n",
    "            ema_l1 = 0\n",
    "            ema_a_recall = 0\n",
    "            ema_d_recall = 0\n",
    "            n_steps_completed = 0\n",
    "            outputs_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            Y_true_acceptor,Y_true_donor,Y_pred_acceptor,Y_pred_donor=[],[],[],[]\n",
    "            n_accum = 0\n",
    "            for i,(batch_features, targets) in enumerate(loop):\n",
    "                batch_features = batch_features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                \n",
    "                #targets = targets.squeeze()[:, CL_max//2:CL_max//2 + SL]\n",
    "                #targets = targets.swapaxes(1,2)[:, CL_max//2:CL_max//2 + SL,:]\n",
    "                targets = targets[:, :,CL_max//2:CL_max//2 + SL]\n",
    "                outputs = model(batch_features)\n",
    "                #print(l1loss)\n",
    "                outputs = outputs[:, CL_max//2 + 1:CL_max//2 + 1 + SL,:].swapaxes(1,2)\n",
    "                #print(outputs.shape,targets.shape)\n",
    "                train_loss = criterion(outputs,targets)/ NUM_ACCUMULATION_STEPS\n",
    "                \n",
    "                if no_softmax:\n",
    "                    outputs = torch.nn.Softmax(dim=1)(outputs)\n",
    "\n",
    "                if reinforce:\n",
    "                    acceptor_reward = torch.gather(targets_full[:,1,:]-targets_full[:,2,:],1,acceptor_actions)\n",
    "                    donor_reward = torch.gather(targets_full[:,2,:]-targets_full[:,1,:],1,donor_actions)\n",
    "                    if phase == 'train':\n",
    "                        acceptor_acc =  torch.nanmean(torch.sum(acceptor_reward>0,dim=1)/torch.sum(targets_full[:,1,:]>0,dim=1))\n",
    "                        acceptor_acc_avg = acceptor_acc*multiplier + acceptor_acc_avg*(1-multiplier)\n",
    "                        donor_acc =  torch.nanmean(torch.sum(donor_reward>0,dim=1)/torch.sum(targets_full[:,2,:]>0,dim=1))\n",
    "                        donor_acc_avg = donor_acc*multiplier + donor_acc_avg*(1-multiplier)\n",
    "\n",
    "                    acceptor_loss = -torch.mean(torch.sum(acceptor_log_probs * acceptor_reward,dim=1))/ NUM_ACCUMULATION_STEPS\n",
    "                    donor_loss = -torch.mean(torch.sum(donor_log_probs * donor_reward,dim=1))/ NUM_ACCUMULATION_STEPS\n",
    "                    reinforce_loss = acceptor_loss+donor_loss\n",
    "                    train_loss = train_loss + 1e-6*reinforce_loss\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    train_loss.backward()\n",
    "                    n_accum = n_accum+1\n",
    "                    if (n_accum == NUM_ACCUMULATION_STEPS) or (i + 1 == len(loop)):\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2.0)\n",
    "                        optimizer.step()\n",
    "                        n_accum = 0\n",
    "\n",
    "                        if epoch<5:\n",
    "                            warmup.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out_argmax = torch.flatten(torch.argmax(outputs,dim=1))\n",
    "                    target_argmax = torch.flatten(torch.argmax(targets,dim=1))\n",
    "                    a_recall = torch.nanmean((out_argmax[target_argmax==1]==1).type(torch.float32))\n",
    "                    d_recall = torch.nanmean((out_argmax[target_argmax==2]==2).type(torch.float32))\n",
    "\n",
    "                if phase == 'val':\n",
    "                    if lowValidationGPUMem:\n",
    "                        outputs = outputs.detach().cpu().numpy()\n",
    "                        targets_list.extend(np.expand_dims(targets.cpu().numpy(),0))\n",
    "                        outputs_list.extend(np.expand_dims(outputs,0))\n",
    "                    else:\n",
    "                        outputs = outputs.detach()\n",
    "                        outputs_list.extend(outputs.unsqueeze(0))\n",
    "                \n",
    "                \n",
    "\n",
    "                loss = NUM_ACCUMULATION_STEPS*train_loss.item()+loss\n",
    "                loop.set_description('Epoch ({}) {}/{}'.format(phase,epoch + 1, epochs))\n",
    "                n_steps_completed += 1\n",
    "                if i==0:\n",
    "                    ema_loss = NUM_ACCUMULATION_STEPS*train_loss.item()\n",
    "                    if ~a_recall.isnan():\n",
    "                        ema_a_recall = a_recall.cpu().numpy()\n",
    "                    if ~d_recall.isnan():\n",
    "                        ema_d_recall = a_recall.cpu().numpy()\n",
    "                else:\n",
    "                    ema_loss = NUM_ACCUMULATION_STEPS*train_loss.item()*multiplier + ema_loss*(1-multiplier)\n",
    "\n",
    "                    if ~a_recall.isnan():\n",
    "                        ema_a_recall = a_recall.cpu().numpy()*multiplier + ema_a_recall*(1-multiplier)\n",
    "                    if ~d_recall.isnan():\n",
    "                        ema_d_recall= d_recall.cpu().numpy()*multiplier + ema_d_recall*(1-multiplier)\n",
    "                if reinforce:\n",
    "                    if i==0:\n",
    "                        ema_reinforce_loss = NUM_ACCUMULATION_STEPS*reinforce_loss.item()\n",
    "                    else:\n",
    "                        ema_reinforce_loss = NUM_ACCUMULATION_STEPS*reinforce_loss.item()*multiplier + ema_reinforce_loss*(1-multiplier)\n",
    "                    loop.set_postfix(loss=ema_loss, r_a=acceptor_acc_avg.item(),r_d=donor_acc_avg.item(),r_loss=ema_reinforce_loss,a_r = ema_a_recall , d_r=ema_d_recall)\n",
    "                else:\n",
    "                    loop.set_postfix(loss=ema_loss,a_r = ema_a_recall , d_r=ema_d_recall)\n",
    "            if phase == 'val':\n",
    "                if lowValidationGPUMem:\n",
    "                    targets = np.swapaxes(np.vstack(targets_list),1,2)\n",
    "                    outputs = np.swapaxes(np.vstack(outputs_list),1,2)\n",
    "                else:\n",
    "                    targets = torch.transpose(torch.vstack(targets_list),1,2).cpu().numpy()\n",
    "                    outputs = torch.transpose(torch.vstack(outputs_list),1,2).cpu().numpy()\n",
    "\n",
    "                is_expr = (targets.sum(axis=(1,2)) >= 1)\n",
    "                Y_true_acceptor.extend(targets[is_expr, :, 1].flatten())\n",
    "                Y_true_donor.extend(targets[is_expr, :, 2].flatten())\n",
    "                Y_pred_acceptor.extend(outputs[is_expr, :, 1].flatten())\n",
    "                Y_pred_donor.extend(outputs[is_expr, :, 2].flatten())\n",
    "\n",
    "            loss = loss / (n_steps_completed)\n",
    "            losses[phase].append(loss)\n",
    "            \n",
    "            if phase == 'val':\n",
    "                Y_true_acceptor, Y_pred_acceptor,Y_true_donor, Y_pred_donor = np.array(Y_true_acceptor), np.array(Y_pred_acceptor),np.array(Y_true_donor), np.array(Y_pred_donor)\n",
    "                if continous_labels:\n",
    "                    Y_true_acceptor = np.round_(Y_true_acceptor,decimals=0)\n",
    "                    Y_true_donor = np.round_(Y_true_donor,decimals=0)\n",
    "                print(\"\\n\\033[1m{}:\\033[0m\".format('Acceptor'))\n",
    "                acceptor_val_results = print_topl_statistics(Y_true_acceptor, Y_pred_acceptor)\n",
    "                print(\"\\n\\033[1m{}:\\033[0m\".format('Donor'))\n",
    "                donor_val_results =print_topl_statistics(Y_true_donor, Y_pred_donor)\n",
    "                val_results.append([acceptor_val_results,donor_val_results])\n",
    "                \n",
    "            \n",
    "            if verbose == 1:\n",
    "                print(\"epoch: {}/{}, {} loss = {:.6f}\".format(epoch + 1, epochs, phase, loss))\n",
    "            if phase == 'val' or skipValidation:\n",
    "                torch.save(model.state_dict(), fileName)\n",
    "        if epoch>=5:\n",
    "            scheduler.step()\n",
    "        \n",
    "    if skipValidation:\n",
    "        return pd.DataFrame({'loss':losses['train']})\n",
    "    else:\n",
    "        return pd.DataFrame({'loss':losses['train'],'val_loss':losses['val']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/100:   5%|████▉                                                                                           | 138/2716 [01:27<20:21,  2.11it/s, a_r=0.00459, d_r=0.00407, loss=0.476]"
     ]
    }
   ],
   "source": [
    "gamma=0.5\n",
    "model_nr = 0\n",
    "epochs=100\n",
    "NUM_ACCUMULATION_STEPS=1\n",
    "modelFileName = '../Results/PyTorch_Models/Hyena_32k_301123_{}'.format(model_nr)\n",
    "#loss = categorical_crossentropy_2d().loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs//4, eta_min=0, last_epoch=-1)\n",
    "warmup = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\n",
    "h = trainModel(model,modelFileName,loss,train_loader,val_loader,optimizer,scheduler,warmup,batch_size,epochs,device,reinforce=False,no_softmax=True,lowValidationGPUMem=True,skipValidation=False,NUM_ACCUMULATION_STEPS=NUM_ACCUMULATION_STEPS,CL_max=CL_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.semilogy(range(epochs),h['loss'],label='Train')\n",
    "plt.semilogy(range(epochs),h['val_loss'],label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45861add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): HyenaDNAModel(\n",
       "    (backbone): LMBackbone(\n",
       "      (embeddings): GPT2Embeddings(\n",
       "        (word_embeddings): Embedding(16, 256)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): Block(\n",
       "          (mixer): HyenaOperator(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "            (filter_fn): HyenaFilter(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (pos_emb): PositionalEmbedding()\n",
       "              (implicit_filter): Sequential(\n",
       "                (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "                (1): Sin()\n",
       "                (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (3): Sin()\n",
       "                (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (5): Sin()\n",
       "                (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "              )\n",
       "              (modulation): ExponentialModulation()\n",
       "            )\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (drop_f): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (head): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment settings:\n",
    "num_epochs = 100  # ~100 seems fine\n",
    "#learning_rate = 6e-4  # good default for Hyena\n",
    "learning_rate = 1e-4 \n",
    "#learning_rate= 1e-3\n",
    "#weight_decay = 0.1\n",
    "ngpu = 3\n",
    "batch_size = ngpu*2*16\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=32, pin_memory=True)\n",
    "\n",
    "\n",
    "#model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "#    '/odinn/tmp/benediktj/',\n",
    "#    pretrained_model_name,\n",
    "#    config=None,\n",
    "#    device=device,\n",
    "#    use_head=use_head,\n",
    "#    n_classes=n_classes,\n",
    "#)\n",
    "\n",
    "# we need these for the decoder head, if using\n",
    "use_head = True\n",
    "n_classes = 3\n",
    "\n",
    "# loss function\n",
    "def loss_fn(output, target):\n",
    "    # output shape: bs, seql+2, n_classes\n",
    "    # target shape: bs, seql\n",
    "    \n",
    "    # only the window ----| SL |----\n",
    "    output = output[:, CL_max//2 + 1:CL_max//2 + 1 + SL]\n",
    "    target = target[:, CL_max//2:CL_max//2 + SL]\n",
    "    print(output.shape,target.shape)\n",
    "    # cross entropy expects (N, C, d1, d2, ...)\n",
    "    output = torch.moveaxis(output, -1, 1)\n",
    "    \n",
    "    # output shape: bs, n_classes, SL\n",
    "    # target shape: bs, SL\n",
    "    ce = torch.nn.functional.cross_entropy(output, target)\n",
    "    loss = torch.mean(ce)\n",
    "    return loss\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# create optimizer\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "        #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#for epoch in range(num_epochs):\n",
    "#    train(model, device, train_loader, optimizer, epoch, loss_fn)\n",
    "#    test(model, device, val_loader, loss_fn)\n",
    "#   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1b6e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a8cecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 1/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2716/2716 [21:49<00:00,  2.07it/s, a_r=2.83e-5, d_r=1.42e-12, loss=0.00283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/10, train loss = 0.126638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 1/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 280/280 [02:26<00:00,  1.91it/s, a_r=0, d_r=0, loss=0.00334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.0017\t0.0025\t0.0073\t0.0191\t0.0011\t0.0970\t0.0582\t0.0332\t0.0202\t53\t21432.0\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.0\t0.0\t0.0\t0.0\t0.0004\t0.9327\t0.8559\t0.6418\t0.4247\t0\t21432.0\t21432\n",
      "epoch: 1/10, val loss = 0.002919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 2/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 2716/2716 [21:50<00:00,  2.07it/s, a_r=0.00571, d_r=1.37e-9, loss=0.00278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2/10, train loss = 0.002842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 2/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 280/280 [02:31<00:00,  1.85it/s, a_r=0.0118, d_r=0.000374, loss=0.00334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.0461\t0.0445\t0.0777\t0.1366\t0.0154\t0.2941\t0.1508\t0.0739\t0.0367\t953\t21432.0\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.0\t0.0\t0.0\t0.0\t0.0006\t0.9983\t0.9975\t0.9963\t0.9943\t0\t21432.0\t21432\n",
      "epoch: 2/10, val loss = 0.002882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2716/2716 [21:48<00:00,  2.08it/s, a_r=0.0115, d_r=4.11e-5, loss=0.0033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/10, train loss = 0.002927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 280/280 [02:32<00:00,  1.84it/s, a_r=0.00356, d_r=0, loss=0.00388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.0844\t0.0728\t0.1154\t0.1819\t0.0268\t0.0279\t0.0122\t0.0057\t0.0026\t1560\t21432.0\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.001\t0.0006\t0.0007\t0.0007\t0.0006\t0.0533\t0.0436\t0.0388\t0.0344\t13\t21432.0\t21432\n",
      "epoch: 3/10, val loss = 0.003353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2716/2716 [21:46<00:00,  2.08it/s, a_r=0.00633, d_r=6.67e-5, loss=0.0037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4/10, train loss = 0.003257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 4/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 280/280 [02:37<00:00,  1.78it/s, a_r=0.0117, d_r=0, loss=0.00441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mAcceptor:\u001b[0m\n",
      "0.084\t0.066\t0.1055\t0.16\t0.0259\t0.0624\t0.0208\t0.0070\t0.0024\t1414\t21432.0\t21432\n",
      "\n",
      "\u001b[1mDonor:\u001b[0m\n",
      "0.0\t0.0\t0.0\t0.0\t0.0005\t0.3799\t0.2967\t0.2661\t0.2414\t0\t21432.0\t21432\n",
      "epoch: 4/10, val loss = 0.003822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (train) 5/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2716/2716 [21:49<00:00,  2.07it/s, a_r=0.000585, d_r=4.45e-5, loss=0.00386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/10, train loss = 0.003537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (val) 5/10:  70%|███████████████████████████████████████████████████████████████████████▋                               | 195/280 [01:50<00:40,  2.08it/s, a_r=0.000109, d_r=0, loss=0.00377]"
     ]
    }
   ],
   "source": [
    "gamma=0.5\n",
    "model_nr = 0\n",
    "epochs=10\n",
    "NUM_ACCUMULATION_STEPS=1\n",
    "modelFileName = '../Results/PyTorch_Models/Hyena_32k_low_lr_161123_{}'.format(model_nr)\n",
    "#loss = categorical_crossentropy_2d().loss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "warmup = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\n",
    "h = trainModel(model,modelFileName,loss,train_loader,val_loader,optimizer,scheduler,warmup,batch_size,epochs,device,reinforce=False,no_softmax=True,lowValidationGPUMem=True,skipValidation=False,NUM_ACCUMULATION_STEPS=NUM_ACCUMULATION_STEPS,CL_max=CL_max)\n",
    "#hs.append(h)\n",
    "\n",
    "plt.semilogy(range(epochs),h['loss'],label='Train')\n",
    "plt.semilogy(range(epochs),h['val_loss'],label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0aad2a-6ac1-4d34-8a76-9e4328af7f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings: torch.Size([16, 32768, 3])\n",
      "Y: torch.Size([16, 32766])\n"
     ]
    }
   ],
   "source": [
    "X,Y = next(iter(val_loader))\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    embeddings = model(X.to(device))\n",
    "\n",
    "print('embeddings:', embeddings.shape)\n",
    "print('Y:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a56164-5203-4cfc-8cea-da17504f1610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = embeddings[:,1:-1].cpu().argmax(axis=2)\n",
    "Y_hat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9433d72-683d-442c-bd13-50d1f34c9d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32766])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Y != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d926d-593a-461d-81e8-79ccbcaeaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = Y_hat[:,1:-1].cpu() == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93778d6-bd15-49ec-983d-55834a770ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(524005)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243770c-8bd5-45b3-8e85-1020d5e6d95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(251)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~true).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f87f66-f71b-49a2-8057-acb7b95f4baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat[:,1:-1].cpu().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06183fc-40a8-428a-96fd-844912099c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
